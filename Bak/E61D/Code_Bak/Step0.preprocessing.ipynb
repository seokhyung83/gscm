{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incorporated-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os, random, time\n",
    "import prophet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy.stats import mode\n",
    "#from datetime import date\n",
    "#import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thermal-camcorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '4_P34_생산실적_2020.csv', '5_P34_생산계획_210405.csv', '6_P34_재고실적_210415.csv', 'DMF0035_MktFcstWaterfall(Mkt Fcst Waterfall)_20210412132805.csv', 'DMF0035_MktFcstWaterfall(Mkt Fcst Waterfall)_20210412174616.csv', 'X_Product.csv', 'X_Sales.csv', 'Y_Inven.csv']\n"
     ]
    }
   ],
   "source": [
    "f_list = os.listdir('./data/')\n",
    "f_list.sort()\n",
    "print(f_list)\n",
    "dt_info = np.vstack((np.column_stack((np.repeat(2020,53),np.arange(1,54))), np.column_stack((np.repeat(2021,21),np.arange(1,22)))))\n",
    "end_info= 66 ## '21 WW13'\n",
    "\n",
    "def last_calendar_week(year):\n",
    "    _year, week, _day = datetime.date(year, 12, 31).isocalendar()\n",
    "    return week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adapted-puppy",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ce21489733f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtmp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euc-kr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthousands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#tmp_title = ['DATE', 'MONTH', 'WW','DAY', 'IN_CELL', 'OUT_CELL', 'INVENTORY_CELL', 'IN_PACK_D', 'OUT_PACK_D', 'INVENTORY_PACK_D']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tmp_Y_inven = pd.DataFrame({'YEAR' : tmp_file['DATE'].str.slice(0,4).astype(int), #pd.to_datetime(tmp_file['DATE']).dt.isocalendar().year,\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0;34m'WW'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misocalendar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweek\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0;34m'DATE'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'"
     ]
    }
   ],
   "source": [
    "#### 재고실적 ####\n",
    "tmp_file = pd.read_csv('./data/'+f_list[2], encoding='euc-kr', thousands=',')\n",
    "#tmp_title = ['DATE', 'MONTH', 'WW','DAY', 'IN_CELL', 'OUT_CELL', 'INVENTORY_CELL', 'IN_PACK_D', 'OUT_PACK_D', 'INVENTORY_PACK_D']                \n",
    "tmp_Y_inven = pd.DataFrame({'YEAR' : tmp_file['DATE'].str.slice(0,4).astype(int), #pd.to_datetime(tmp_file['DATE']).dt.isocalendar().year,\n",
    "                            'WW' : pd.to_datetime(tmp_file['DATE'], format='%Y-%m-%d').dt.isocalendar().week,\n",
    "                            'DATE' : tmp_file['DATE'],\n",
    "                            'INVENTORY_PACK_D' : tmp_file['INVENTORY_PACK_D']})\n",
    "\n",
    "#tmp_inven_qty = list()\n",
    "for i in range(0, end_info):\n",
    "    \n",
    "    if last_calendar_week(dt_info[i][0]) == dt_info[i][1]:    \n",
    "        tmp_y_dta = tmp_Y_inven[(tmp_Y_inven['WW']==dt_info[i][1])] #(tmp_Y_inven['YEAR']==dt_info[i][0]) & \n",
    "    else:\n",
    "        tmp_y_dta = tmp_Y_inven[(tmp_Y_inven['YEAR']==dt_info[i][0]) & (tmp_Y_inven['WW']==dt_info[i][1])] #\n",
    "\n",
    "    #tmp_inven_qty.append(tmp_y_dta['INVENTORY_PACK_D'].mean())\n",
    "    d_week = np.array(pd.to_datetime(tmp_y_dta['DATE'], format='%Y-%m-%d').dt.dayofweek)\n",
    "    #tmp_y_dta['INVENTORY_PACK_D']#.iloc[0,:]#[0, (np.where(d_week == 6)[0][0]-1)]\n",
    "    for j in range(0 ,len(d_week)):\n",
    "        if d_week[j] == 6:\n",
    "            tmp_inven_qty = tmp_y_dta['INVENTORY_PACK_D'].iloc[j]\n",
    "    tmp_inven_qty = np.array((dt_info[i][0], dt_info[i][1], tmp_inven_qty))\n",
    "    if i==0:\n",
    "        inven_dta_qty = tmp_inven_qty\n",
    "    else:\n",
    "        inven_dta_qty = np.vstack((inven_dta_qty, tmp_inven_qty))\n",
    "    #print(dt_info[i], tmp_y_dta.shape)\n",
    "cnt = 1\n",
    "for i in range(9, end_info):\n",
    "    tmp_inven_qty = []\n",
    "    for j in range(0, 9):\n",
    "        tmp_inven_qty.append(inven_dta_qty[(i-j),2])\n",
    "    if cnt ==1 :\n",
    "        tmp_inven_dta = tmp_inven_qty\n",
    "    else:\n",
    "        tmp_inven_dta = np.vstack((tmp_inven_dta, tmp_inven_qty))\n",
    "    cnt+=1\n",
    "    \n",
    "#inven_dta = pd.DataFrame(dt_info[9:end_info])\n",
    "##inven_dta['INVENTORY_PACK_D'] = tmp_inven_qty\n",
    "inven_dta = pd.DataFrame(dt_info[9:end_info])\n",
    "inven_dta = pd.concat([inven_dta, pd.DataFrame(tmp_inven_dta)], axis=1)\n",
    "#inven_dta \n",
    "inven_dta.columns = list(['YEAR', 'WW']) + list(map(lambda x : \"INVENTORY\" if x == 0  else \"prev_INVENTORY_\"+str(x), range(0, 9))) #list(['YEAR', 'WW', 'INVENTORY_PACK_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "meaning-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "inven_dta.to_csv('./data/Y_Inven.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-northwest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nasty-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 생산실적 ####\n",
    "tmp_file = pd.read_csv('./data/'+f_list[0], encoding='euc-kr', thousands=',')\n",
    "tmp_title = ['Prod Type','Material', 'Basic Date', 'Plan Date', 'Plan Qty', 'Result Qty']\n",
    "tmp_p_real1 = tmp_file[tmp_title].copy()\n",
    "tmp_p_real1['YEAR'] = pd.to_datetime(tmp_file['Plan Date']).dt.isocalendar().year\n",
    "tmp_p_real1['WW'] = pd.to_datetime(tmp_file['Plan Date']).dt.isocalendar().week\n",
    "X_product_actual = tmp_p_real1[(tmp_p_real1['YEAR']==2020)& (tmp_p_real1['Material']=='APVCCCMA0-A2-A01')]  #(tmp_p_real1['Prod Type']=='MASS') & \n",
    "\n",
    "cnt = 0\n",
    "r_QTY_sum = list()\n",
    "for t in range(0, end_info): #53):# \n",
    "    TW=dt_info[t,1]; TY=dt_info[t,0]    \n",
    "    if any((X_product_actual['YEAR']==TY) & (X_product_actual['WW']==TW)):\n",
    "        r_QTY_sum.append(X_product_actual[(X_product_actual['YEAR']==TY) & (X_product_actual['WW']==TW)]['Result Qty'].sum())\n",
    "    else:\n",
    "        r_QTY_sum.append(None)\n",
    "\n",
    "        \n",
    "################################################################################################\n",
    "r_QTY_sum[53:66] = [842,1285,1254,1102,1274,1243,1327,870,1077,1351,1170,837,992]\n",
    "################################################################################################\n",
    "#product_dta['Products'] = r_QTY_sum\n",
    "\n",
    "#### 생산계획 ####\n",
    "tmp_file = pd.read_csv('./data/'+f_list[1], thousands=',')\n",
    "tmp_title = ['PLAN_START_YYYYMMDD','YYYYMMDD', 'QTY']\n",
    "X_product_plan = tmp_file[tmp_title].copy() \n",
    "X_product_plan['PLAN_WW'] = pd.to_datetime(tmp_file['PLAN_START_YYYYMMDD'], format='%Y%m%d').dt.isocalendar().week\n",
    "X_product_plan['PLAN_YEAR'] = pd.to_datetime(tmp_file['PLAN_START_YYYYMMDD'], format='%Y%m%d').dt.isocalendar().year\n",
    "X_product_plan['TARGET_WW'] = pd.to_datetime(tmp_file['YYYYMMDD'], format='%Y%m%d').dt.isocalendar().week\n",
    "X_product_plan['TARGET_YEAR'] = pd.to_datetime(tmp_file['YYYYMMDD'], format='%Y%m%d').dt.isocalendar().year\n",
    "X_product_plan.head()\n",
    "\n",
    "prev_name = list(map(lambda x : 'Prev_Product_'+str(x), np.arange(1, 9, 1)))\n",
    "post_name = list(map(lambda x : 'Post_Product_'+str(x), np.arange(1, 9, 1)))\n",
    "var_name  = list(map(lambda x : 'Var_Product_'+str(x),  np.arange(1, 9, 1)))\n",
    "\n",
    "cnt = 0\n",
    "for t in range(9, end_info): #53):# \n",
    "    QTY_prev = list()\n",
    "    QTY_post = list()    \n",
    "    QTY_var = list()    \n",
    "    \n",
    "    for j in range(1, 9):        \n",
    "        target_y = dt_info[t][0] ; target_w = dt_info[t][1]\n",
    "        post_y = dt_info[t+j][0]; post_w = dt_info[t+j][1]; \n",
    "        #prev_y = dt_info[t-j][0]; prev_w = dt_info[t-j][1];        \n",
    "        #print(post_y,post_w, \" / \", prev_y, prev_w)\n",
    "        #QTY_prev.append(X_product_plan[(X_product_plan['PLAN_YEAR']==prev_y) & (X_product_plan['PLAN_WW']==prev_w) & (X_product_plan['TARGET_YEAR']==target_y) & (X_product_plan['TARGET_WW']==target_w)]['QTY'].sum())        \n",
    "        QTY_prev.append(r_QTY_sum[t-j])\n",
    "        QTY_post.append(X_product_plan[(X_product_plan['PLAN_YEAR']==target_y) & (X_product_plan['PLAN_WW']==target_w) & (X_product_plan['TARGET_YEAR']==post_y) & (X_product_plan['TARGET_WW']==post_w)]['QTY'].sum())\n",
    "\n",
    "    for j in range(1, 9):        \n",
    "        target_y = dt_info[t][0] ; target_w = dt_info[t][1]\n",
    "        prev_y = dt_info[t-j][0]; prev_w = dt_info[t-j][1];        \n",
    "        #print(post_y,post_w, \" / \", prev_y, prev_w)\n",
    "        QTY_var.append(X_product_plan[(X_product_plan['PLAN_YEAR']==prev_y) & (X_product_plan['PLAN_WW']==prev_w) & (X_product_plan['TARGET_YEAR']==target_y) & (X_product_plan['TARGET_WW']==target_w)]['QTY'].sum())\n",
    "\n",
    "    tmp_QTY = QTY_prev + QTY_post + QTY_var   \n",
    "    if cnt== 0:\n",
    "        tmp_X_product_plan = tmp_QTY\n",
    "    else:\n",
    "        tmp_X_product_plan = np.vstack((tmp_X_product_plan, tmp_QTY))\n",
    "    cnt+=1\n",
    "    \n",
    "product_dta = pd.DataFrame(tmp_X_product_plan)\n",
    "product_dta.columns = list(prev_name+post_name+var_name)\n",
    "product_dta['YEAR'] = dt_info[9:end_info,0]\n",
    "product_dta['WW'] = dt_info[9:end_info,1]\n",
    "product_dta['Products'] = r_QTY_sum[9:end_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "serial-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dta= product_dta.fillna(-999)\n",
    "product_dta= product_dta.replace(0, -999)\n",
    "product_dta.to_csv('./data/X_Product.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-advertiser",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deadly-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 판매계획/실적 Data Set ####\n",
    "def dta_merge(t_tmp, t_title):\n",
    "    #t_tmp = tmp1 ; t_title = tmp1.columns\n",
    "    ver_col = np.where(t_tmp.columns == 'Version')[0][0]\n",
    "    t_tmp1 = pd.DataFrame(t_tmp.iloc[:,ver_col].copy())\n",
    "    t_tmp1 = pd.concat([t_tmp1,t_tmp.iloc[:, 14:]], axis=1)\n",
    "    return t_tmp1\n",
    "\n",
    "\n",
    "tmp1_title = pd.read_csv('./data/'+f_list[3], skiprows=0, nrows=1)#, header=None)\n",
    "tmp1 = pd.read_csv('./data/'+f_list[3], skiprows=2, thousands=',')\n",
    "#tmp1.columns = tmp1_title.loc[0]\n",
    "#tmp1_dta = column_merge(tmp1, tmp1_title)\n",
    "\n",
    "tmp2_title = pd.read_csv('./data/'+f_list[4], skiprows=0, nrows=1)#, header=None)\n",
    "tmp2 = pd.read_csv('./data/'+f_list[4], skiprows=2, thousands=',')\n",
    "#tmp1_sub.columns = tmp1_sub_title.loc[0]\n",
    "#tmp1_sub_dta = column_merge(tmp1_sub, tmp1_sub_title)\n",
    "\n",
    "tmp1.columns = tmp1_title.loc[0]\n",
    "tmp2.columns = tmp2_title.loc[0]\n",
    "\n",
    "tmp1_name = list(map(lambda i : str('Y')+tmp1_title.columns[i].split('-')[0]+'_'+tmp1_title.iloc[0,i], range(14, len(tmp1_title.loc[0]))))\n",
    "tmp2_name = list(map(lambda i : str('Y')+tmp2_title.columns[i].split('-')[0]+'_'+tmp2_title.iloc[0,i], range(14, len(tmp2_title.loc[0]))))\n",
    "tmp1.columns = list(tmp1.columns[:14]) + tmp1_name\n",
    "tmp2.columns = list(tmp2.columns[:14]) + tmp2_name\n",
    "\n",
    "tmp1 = dta_merge(tmp1, tmp1.columns)\n",
    "tmp2 = dta_merge(tmp2, tmp2.columns)\n",
    "\n",
    "prev_name = list(map(lambda x : 'Prev_Sale_'+str(x), np.arange(8, 0, -1)))\n",
    "post_name = list(map(lambda x : 'Post_Sale_'+str(x), np.arange(1, 9,  1)))\n",
    "var_name  = list(map(lambda x : 'Var_Sale_'+str(x), np.arange(1, 9,  1)))\n",
    "\n",
    "sales_x = []\n",
    "sales_y = []\n",
    "\n",
    "for l in range(9, end_info):#dt_info.shape[0]):\n",
    "    max_week = last_calendar_week(dt_info[l][0])\n",
    "    check_week = str('Y')+str(dt_info[l][0])+'_'+str(max_week)\n",
    "    if (dt_info[l][1] + 8 < max_week) & (dt_info[l][0] == 2020):\n",
    "        tmp_dta = tmp1\n",
    "        tmp_name = list(map(lambda i : str('Y')+tmp1_title.columns[i].split('-')[0]+'_'+tmp1_title.iloc[0,i], range(14, len(tmp1_title.loc[0]))))\n",
    "        #print('tmp1')\n",
    "    else:\n",
    "        tmp_dta = tmp2\n",
    "        tmp_name = list(map(lambda i : str('Y')+tmp2_title.columns[i].split('-')[0]+'_'+tmp2_title.iloc[0,i], range(14, len(tmp2_title.loc[0]))))\n",
    "        if 'Y2021_W53' in tmp_name :\n",
    "            re_col = tmp_name.index('Y2021_W53')#np.where('Y2021_W53' == tmp_name)[0]\n",
    "            tmp_name[re_col] = 'Y2020_W53'        \n",
    "        #print('tmp2')\n",
    "\n",
    "    if check_week in tmp_name:\n",
    "        check_index = tmp_name.index(check_week)\n",
    "        tmp_name[check_index] = (str('Y')+str(dt_info[l][0])+'_W'+str(max_week))\n",
    "    tmp_dta.columns = list(tmp_dta.columns[:1]) + tmp_name\n",
    "    #print(tmp_name)\n",
    "    tmp_row_y = pd.to_datetime(tmp_dta['Version'].str.slice(5,13), format='%Y%m%d').dt.isocalendar().year\n",
    "    tmp_row_w = pd.to_datetime(tmp_dta['Version'].str.slice(5,13), format='%Y%m%d').dt.isocalendar().week\n",
    "    #print(l, dt_info[l])\n",
    "    current_no = np.where((dt_info[l][0] == tmp_row_y) & (dt_info[l][1] == tmp_row_w))[0][0]\n",
    "    prev_no = l - 8\n",
    "    post_no = (l+1) + 8\n",
    "\n",
    "    prev_x = list(map(lambda i : (str('Y')+str(dt_info[i][0])+'_W0'+str(dt_info[i][1])) if np.divmod(dt_info[i][1], 10)[0] == 0 else (str('Y')+str(dt_info[i][0])+'_W'+str(dt_info[i][1])), range(prev_no, l)))\n",
    "    post_x = list(map(lambda i : (str('Y')+str(dt_info[i][0])+'_W0'+str(dt_info[i][1])) if np.divmod(dt_info[i][1], 10)[0] == 0 else (str('Y')+str(dt_info[i][0])+'_W'+str(dt_info[i][1])), range((l+1), post_no)))\n",
    "    current_x = (str('Y')+str(dt_info[l][0])+'_W0'+str(dt_info[l][1])) if np.divmod(dt_info[l][1], 10)[0] == 0 else (str('Y')+str(dt_info[l][0])+'_W'+str(dt_info[l][1]))\n",
    "    \n",
    "    tmp_sale_list = []\n",
    "    tmp_col = np.where(tmp_dta.columns == current_x)[0]\n",
    "    if (current_no+1) < tmp_dta.shape[0]:\n",
    "        if len(tmp_col) == 1:        \n",
    "            tmp_sale_list.append(mode(tmp_dta.iloc[(current_no+1):, tmp_col]).mode[0][0])\n",
    "        else:        \n",
    "            tmp_sale_list.append(mode(tmp_dta.iloc[(current_no+1):, tmp_col].fillna(0).sum(axis=1)).mode[0])\n",
    "    else:\n",
    "        tmp_sale_list.append(0)\n",
    "\n",
    "    tmp_prev_list = []\n",
    "    for i in prev_x:        \n",
    "        tmp_col = np.where(tmp_dta.columns == i)[0]\n",
    "        if len(tmp_col) == 1:\n",
    "            tmp_prev_list.append(tmp_dta.iloc[current_no, tmp_col].values[0])        \n",
    "        else:\n",
    "            tmp_prev_list.append(tmp_dta.iloc[current_no, tmp_col].fillna(0).sum())\n",
    "\n",
    "    tmp_post_list = []\n",
    "    for i in post_x:        \n",
    "        tmp_col = np.where(tmp_dta.columns == i)[0]\n",
    "        if len(tmp_col) == 1:\n",
    "            tmp_post_list.append(tmp_dta.iloc[current_no, tmp_col].values[0])        \n",
    "        else:\n",
    "            tmp_post_list.append(tmp_dta.iloc[current_no, tmp_col].fillna(0).sum())\n",
    "\n",
    "    tmp_var_list = []\n",
    "    for i in range(0, 8):    \n",
    "        tmp_col = np.where(tmp_dta.columns == current_x)[0]        \n",
    "        if len(tmp_col) == 1:\n",
    "            tmp_var_list.append(tmp_dta.iloc[((current_no-1) - i), tmp_col].values[0])\n",
    "        else:\n",
    "            tmp_var_list.append(tmp_dta.iloc[((current_no-1) - i), tmp_col].fillna(0).sum())\n",
    "    #print(tmp_var_list)\n",
    "    tmp_x1 = list(tmp_prev_list + tmp_post_list + tmp_var_list)\n",
    "    tmp_x2 = tmp_sale_list[0]\n",
    "    sales_x.append(tmp_x1)\n",
    "    sales_y.append(tmp_x2)\n",
    "sales_dta = pd.DataFrame(sales_x)\n",
    "\n",
    "sales_dta.columns = list(prev_name+post_name+var_name)#+['Sale'])\n",
    "sales_dta['YEAR'] = dt_info[9:end_info, 0]\n",
    "sales_dta['WW'] = dt_info[9:end_info,1]\n",
    "sales_dta['Sales'] = sales_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "derived-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dta = sales_dta.fillna(-999)\n",
    "sales_dta = sales_dta.replace(0, -999)\n",
    "sales_dta = sales_dta.replace(-2, -999)\n",
    "sales_dta = sales_dta.replace(-4, -999)\n",
    "sales_dta.to_csv('./data/X_Sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brilliant-quarterly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MF_W_20200303_V001'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.to_datetime(tmp1['Version'].str.slice(5,13), format='%Y%m%d').dt.isocalendar().week\n",
    "tmp1['Version'].loc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-raise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

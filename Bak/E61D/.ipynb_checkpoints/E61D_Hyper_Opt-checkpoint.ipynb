{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e983384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "import xgboost\n",
    "import datetime\n",
    "import pygam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "#from fbprophet import Prophet\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.kernel_ridge import KernelRidge as KR\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, Lars, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel as C, RBF\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b219a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inven = pd.read_csv('./E61D_Inven.csv')\n",
    "x_sales = pd.read_csv('./E61D_Sale.csv')\n",
    "x_product = pd.read_csv('./E61D_Product.csv')\n",
    "d_set = pd.merge(pd.merge(y_inven, x_sales, how='left', on='Week'), x_product, how='left', on='Week')\n",
    "d_set = d_set.drop(columns=['5xxx', '3xxC', '3xxT'])\n",
    "d_set[d_set < -1000] = 0\n",
    "#d_set_log = d_set.copy()\n",
    "#d_set_log[d_set < 0] = 0\n",
    "#d_set.to_csv('data_0506.csv', index=False)\n",
    "\n",
    "rm_week = [201952, 202001, 202052, 202053, 202101]\n",
    "rm_index = [i for i,s in enumerate(d_set['Week']) if s not in rm_week]\n",
    "d_set1 = d_set.loc[rm_index]\n",
    "d_set1 = d_set1.reset_index(drop=True)\n",
    "#print(d_set.shape)\n",
    "#d_set1.to_csv('data_0506.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e2ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01bc099",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_model_1(m_, x_dta, wt_mat, t_col_k, s_col_k, p_col_k, tr_s_day, tr_e_day, te_s_day=None, te_e_day=None):    \n",
    "    tr_time = np.array([tr_s_day, tr_e_day])\n",
    "    te_time = np.array([te_s_day, te_e_day])\n",
    "    s_ind, e_ind = np.where(tr_time[0] == x_dta['Week'])[0][0], np.where(tr_time[1] == x_dta['Week'])[0][0]\n",
    "    te_s_ind, te_e_ind = np.where(te_time[0] == x_dta['Week'])[0][0], np.where(te_time[1] == x_dta['Week'])[0][0]\n",
    "\n",
    "    target_col = [s for s in x_dta.columns.values if 'Inven' == s]\n",
    "    week_col = [s for s in x_dta.columns.values if 'Week' in s]\n",
    "\n",
    "    prev_target_col = [s for s in x_dta.columns.values if 'Prev_Inven' in s]\n",
    "    var_sale_col = [s for s in x_dta.columns.values if 'Var_Sale' in s]\n",
    "    var_product_col = [s for s in x_dta.columns.values if 'Var_Product' in s]\n",
    "    sale_col = [s for s in x_dta.columns.values if 'Sale' == s]\n",
    "    product_col = [s for s in x_dta.columns.values if 'Product' == s]\n",
    "\n",
    "    tmp_x_set1, tmp_x_set2, tmp_x_set3 = x_dta[prev_target_col], x_dta[var_sale_col], x_dta[var_product_col]    \n",
    "    tmp_x_set1 = np.multiply(tmp_x_set1, np.tile([wt_mat[:tmp_x_set1.shape[1]]], tmp_x_set1.shape[0]).reshape(tmp_x_set1.shape[0], -1)).copy()\n",
    "    tmp_x_set2 = np.multiply(tmp_x_set2, np.tile([wt_mat], tmp_x_set2.shape[0]).reshape(tmp_x_set2.shape[0], -1)).copy()\n",
    "    tmp_x_set3 = np.multiply(tmp_x_set3, np.tile([wt_mat], tmp_x_set3.shape[0]).reshape(tmp_x_set3.shape[0], -1)).copy()\n",
    "\n",
    "    tmp_x_set1_re = tmp_x_set1.iloc[:,:t_col_k]\n",
    "    tmp_x_set2_re = tmp_x_set2.iloc[:,:s_col_k]\n",
    "    tmp_x_set3_re = tmp_x_set3.iloc[:,:p_col_k]\n",
    "\n",
    "    x_dta_re = pd.concat([x_dta[list(week_col+target_col+sale_col+product_col)], tmp_x_set1_re, tmp_x_set2_re, tmp_x_set3_re], axis=1)\n",
    "\n",
    "    train_set = x_dta_re.iloc[s_ind:e_ind, :]\n",
    "    test_set = x_dta_re.iloc[te_s_ind:te_e_ind, :]#.copy()\n",
    "\n",
    "    tr_x_set = train_set[list(prev_target_col[:t_col_k]+sale_col+var_sale_col[:s_col_k]+product_col+var_product_col[:p_col_k])]\n",
    "    tr_y_set = np.array(train_set[target_col])\n",
    "    te_x_set = test_set[list(prev_target_col[:t_col_k]+sale_col+var_sale_col[:s_col_k]+product_col+var_product_col[:p_col_k])]\n",
    "    te_y_set = np.array(test_set[target_col])\n",
    "\n",
    "\n",
    "    m_.fit(tr_x_set, tr_y_set)\n",
    "    hat_prev_ = m_.predict(tr_x_set)\n",
    "    \n",
    "    if len(tr_y_set) > 0 :\n",
    "        tmp_y_id_ = np.where(tr_y_set!=0)[0]\n",
    "        real_ = np.mean(1- np.abs(tr_y_set[tmp_y_id_] - hat_prev_[tmp_y_id_]) / np.abs(tr_y_set[tmp_y_id_])) * 100\n",
    "    else:\n",
    "        real_ = np.mean(1- np.abs(tr_y_set - hat_prev_) / np.abs(tr_y_set)) * 100     \n",
    "\n",
    "    if te_s_day is None :\n",
    "        return real_, tr_y_set, hat_prev_\n",
    "    else:\n",
    "        hat_  = m_.predict(te_x_set)\n",
    "        Y_hat_ = np.concatenate((hat_prev_, hat_))\n",
    "        Y_     = np.concatenate((tr_y_set, te_y_set))\n",
    "        if len(te_y_set) > 0:\n",
    "            tmp_y_id_ = np.where(te_y_set!=0)[0]\n",
    "            fcst_ = np.mean(1- np.abs(te_y_set[tmp_y_id_] - hat_[tmp_y_id_]) / np.abs(te_y_set[tmp_y_id_])) * 100\n",
    "        else:\n",
    "            fcst_ = np.mean(1- np.abs(te_y_set - hat_     ) / np.abs(te_y_set)) * 100    \n",
    "        return real_, fcst_, Y_, Y_hat_, hat_prev_, hat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b4ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_2(m_, x_dta, target_key, wt_mat, tr_s_day, tr_e_day, te_s_day=None, te_e_day=None, col_k=None, col_k1=None):    \n",
    "    \n",
    "    tr_time = np.array([tr_s_day, tr_e_day])\n",
    "    te_time = np.array([te_s_day, te_e_day])\n",
    "    s_ind, e_ind = np.where(tr_time[0] == x_dta['Week'])[0][0], np.where(tr_time[1] == x_dta['Week'])[0][0]\n",
    "    te_s_ind, te_e_ind = np.where(te_time[0] == x_dta['Week'])[0][0], np.where(te_time[1] == x_dta['Week'])[0][0]\n",
    "\n",
    "    target_col = [s for s in x_dta.columns.values if target_key == s]    \n",
    "    tmp_x_col = [s for s in x_dta.columns.values if 'Prev_'+target_key in s]\n",
    "    tmp_x_col1 = [s for s in x_dta.columns.values if 'Var_'+target_key in s]\n",
    "    week_col = [s for s in x_dta.columns.values if 'Week' in s]\n",
    "\n",
    "    tmp_x_set1, tmp_x_set2 = x_dta[tmp_x_col], x_dta[tmp_x_col1]\n",
    "    tmp_x_set1 = np.multiply(tmp_x_set1, np.tile([wt_mat], tmp_x_set1.shape[0]).reshape(tmp_x_set1.shape[0], -1)).copy()\n",
    "    tmp_x_set2 = np.multiply(tmp_x_set2, np.tile([wt_mat], tmp_x_set2.shape[0]).reshape(tmp_x_set2.shape[0], -1)).copy()\n",
    "\n",
    "    if target_key == 'Product':\n",
    "        tmp_x_set1_re = tmp_x_set1.iloc[:,:col_k]\n",
    "        tmp_x_set2_re = tmp_x_set2.iloc[:,:col_k1]\n",
    "        x_dta_re = pd.concat([x_dta[list(week_col+target_col)], tmp_x_set1_re, tmp_x_set2_re], axis=1)\n",
    "        train_col = list(tmp_x_col[:col_k]+tmp_x_col1[:col_k1])    \n",
    "    elif target_key == 'Sale':\n",
    "        tmp_x_set2_re = tmp_x_set2.iloc[:,:col_k1]\n",
    "        x_dta_re = pd.concat([x_dta[list(week_col+target_col)], tmp_x_set2_re], axis=1)\n",
    "        train_col = list(tmp_x_col1[:col_k1])\n",
    "\n",
    "\n",
    "    train_set = x_dta_re.iloc[s_ind:e_ind, :]\n",
    "    test_set = x_dta_re.iloc[te_s_ind:(te_e_ind+1), :]#.copy()\n",
    "\n",
    "    tr_x_set = train_set[train_col]\n",
    "    tr_y_set = np.array(train_set[target_col]).reshape(-1,1)\n",
    "    te_x_set = test_set[train_col]\n",
    "    te_y_set = np.array(test_set[target_col]).reshape(-1,1)\n",
    "\n",
    "    m_.fit(tr_x_set, tr_y_set)\n",
    "    hat_prev_ = m_.predict(tr_x_set)\n",
    "    if len(np.shape(hat_prev_)) == 1:\n",
    "        hat_prev_ = hat_prev_.reshape(-1,1)\n",
    "\n",
    "    real_ = np.mean(np.ones_like(tr_y_set) - np.clip(np.abs(tr_y_set - hat_prev_) / tr_y_set , 0, 1)) * 100    \n",
    "    #if len(tr_y_set) > 0 :\n",
    "    #    tmp_y_id_ = np.where(tr_y_set!=0)[0]\n",
    "    #    real_ = np.mean(1- np.abs(tr_y_set[tmp_y_id_] - hat_prev_[tmp_y_id_]) / np.abs(tr_y_set[tmp_y_id_])) * 100\n",
    "    #else:\n",
    "    #    real_ = np.mean(1- np.abs(tr_y_set - hat_prev_) / np.abs(tr_y_set)) * 100     \n",
    "\n",
    "\n",
    "    hat_ = []\n",
    "    if target_key == 'Product':\n",
    "        prev_x = list(map(lambda j : x_dta_re[target_key].iloc[te_s_ind-j], range(1, col_k+1)))\n",
    "        for i in range(te_s_ind, te_e_ind+1):    \n",
    "            var_x = x_dta_re[tmp_x_col1[:col_k1]].iloc[i]        \n",
    "            te_x_ = np.array(prev_x+list(var_x)).reshape(1,-1)    \n",
    "            #print(te_x_.shape)\n",
    "            tmp_y_hat_ = m_.predict(te_x_)\n",
    "            if len(np.shape(tmp_y_hat_)) > 1:\n",
    "                tmp_y_hat_ = tmp_y_hat_.reshape(-1, )            \n",
    "            prev_x = (list(tmp_y_hat_) + prev_x)[:col_k]\n",
    "#            if len(np.shape(tmp_y_hat_)) == 1:\n",
    "#                tmp_y_hat_ = tmp_y_hat_.reshape(-1,1)        \n",
    "            hat_.append(tmp_y_hat_)\n",
    "            \n",
    "    elif target_key == 'Sale':\n",
    "        tmp_y_hat_ = m_.predict(te_x_set)\n",
    "        if len(np.shape(tmp_y_hat_)) > 1:\n",
    "            tmp_y_hat_ = tmp_y_hat_.reshape(-1, )             \n",
    "        hat_.append(tmp_y_hat_) \n",
    "\n",
    "    Y_hat_ = np.concatenate((hat_prev_, hat_)) #hat_))#\n",
    "    Y_     = np.concatenate((tr_y_set, te_y_set))\n",
    "    fcst_ = np.mean(np.ones_like(te_y_set) - np.clip(np.abs(te_y_set - hat_[0]) / te_y_set , 0, 1)) * 100    \n",
    "    return real_, fcst_, Y_, Y_hat_, hat_prev_, hat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32ae5f87",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |   m_lr    | m_max_... | m_n_es... | m_subs... |  p_col_k  |  s_col_k  |  t_col_k  |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 69.27   \u001b[0m | \u001b[0m 0.9084  \u001b[0m | \u001b[0m 0.7159  \u001b[0m | \u001b[0m 3.001   \u001b[0m | \u001b[0m 37.21   \u001b[0m | \u001b[0m 0.5587  \u001b[0m | \u001b[0m 2.053   \u001b[0m | \u001b[0m 3.123   \u001b[0m | \u001b[0m 3.557   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 81.15   \u001b[0m | \u001b[95m 0.9055  \u001b[0m | \u001b[95m 0.538   \u001b[0m | \u001b[95m 5.515   \u001b[0m | \u001b[95m 71.67   \u001b[0m | \u001b[95m 0.5818  \u001b[0m | \u001b[95m 11.01   \u001b[0m | \u001b[95m 1.312   \u001b[0m | \u001b[95m 5.961   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 79.32   \u001b[0m | \u001b[0m 0.9084  \u001b[0m | \u001b[0m 0.5575  \u001b[0m | \u001b[0m 3.842   \u001b[0m | \u001b[0m 27.83   \u001b[0m | \u001b[0m 0.8203  \u001b[0m | \u001b[0m 12.04   \u001b[0m | \u001b[0m 4.573   \u001b[0m | \u001b[0m 6.123   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 76.52   \u001b[0m | \u001b[0m 0.9727  \u001b[0m | \u001b[0m 0.8867  \u001b[0m | \u001b[0m 3.51    \u001b[0m | \u001b[0m 13.51   \u001b[0m | \u001b[0m 0.5679  \u001b[0m | \u001b[0m 11.01   \u001b[0m | \u001b[0m 2.121   \u001b[0m | \u001b[0m 4.116   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 81.22   \u001b[0m | \u001b[95m 0.9841  \u001b[0m | \u001b[95m 0.5325  \u001b[0m | \u001b[95m 7.151   \u001b[0m | \u001b[95m 38.4    \u001b[0m | \u001b[95m 0.7746  \u001b[0m | \u001b[95m 10.51   \u001b[0m | \u001b[95m 1.208   \u001b[0m | \u001b[95m 6.551   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 72.84   \u001b[0m | \u001b[0m 0.9884  \u001b[0m | \u001b[0m 0.7432  \u001b[0m | \u001b[0m 4.683   \u001b[0m | \u001b[0m 81.04   \u001b[0m | \u001b[0m 0.5413  \u001b[0m | \u001b[0m 6.106   \u001b[0m | \u001b[0m 11.36   \u001b[0m | \u001b[0m 3.173   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 79.4    \u001b[0m | \u001b[0m 0.8903  \u001b[0m | \u001b[0m 0.1374  \u001b[0m | \u001b[0m 3.116   \u001b[0m | \u001b[0m 71.1    \u001b[0m | \u001b[0m 0.5847  \u001b[0m | \u001b[0m 4.027   \u001b[0m | \u001b[0m 6.604   \u001b[0m | \u001b[0m 1.395   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 80.42   \u001b[0m | \u001b[0m 0.9304  \u001b[0m | \u001b[0m 0.1538  \u001b[0m | \u001b[0m 6.536   \u001b[0m | \u001b[0m 72.98   \u001b[0m | \u001b[0m 0.5409  \u001b[0m | \u001b[0m 5.72    \u001b[0m | \u001b[0m 8.916   \u001b[0m | \u001b[0m 4.065   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 78.61   \u001b[0m | \u001b[0m 0.857   \u001b[0m | \u001b[0m 0.5352  \u001b[0m | \u001b[0m 6.983   \u001b[0m | \u001b[0m 56.34   \u001b[0m | \u001b[0m 0.8778  \u001b[0m | \u001b[0m 7.687   \u001b[0m | \u001b[0m 11.3    \u001b[0m | \u001b[0m 2.017   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 77.39   \u001b[0m | \u001b[0m 0.8695  \u001b[0m | \u001b[0m 0.8012  \u001b[0m | \u001b[0m 5.386   \u001b[0m | \u001b[0m 24.88   \u001b[0m | \u001b[0m 0.871   \u001b[0m | \u001b[0m 4.965   \u001b[0m | \u001b[0m 9.559   \u001b[0m | \u001b[0m 6.372   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 70.86   \u001b[0m | \u001b[0m 0.9737  \u001b[0m | \u001b[0m 0.6212  \u001b[0m | \u001b[0m 7.506   \u001b[0m | \u001b[0m 41.4    \u001b[0m | \u001b[0m 0.608   \u001b[0m | \u001b[0m 11.21   \u001b[0m | \u001b[0m 5.88    \u001b[0m | \u001b[0m 8.14    \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 75.81   \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.6193  \u001b[0m | \u001b[0m 3.688   \u001b[0m | \u001b[0m 95.45   \u001b[0m | \u001b[0m 0.68    \u001b[0m | \u001b[0m 7.594   \u001b[0m | \u001b[0m 5.653   \u001b[0m | \u001b[0m 2.754   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 80.58   \u001b[0m | \u001b[0m 0.9765  \u001b[0m | \u001b[0m 0.5722  \u001b[0m | \u001b[0m 3.017   \u001b[0m | \u001b[0m 65.54   \u001b[0m | \u001b[0m 0.6307  \u001b[0m | \u001b[0m 7.008   \u001b[0m | \u001b[0m 11.1    \u001b[0m | \u001b[0m 3.644   \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 83.0    \u001b[0m | \u001b[95m 0.9772  \u001b[0m | \u001b[95m 0.6209  \u001b[0m | \u001b[95m 3.095   \u001b[0m | \u001b[95m 93.65   \u001b[0m | \u001b[95m 0.7764  \u001b[0m | \u001b[95m 12.37   \u001b[0m | \u001b[95m 2.965   \u001b[0m | \u001b[95m 2.015   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 77.37   \u001b[0m | \u001b[0m 0.9806  \u001b[0m | \u001b[0m 0.6929  \u001b[0m | \u001b[0m 3.396   \u001b[0m | \u001b[0m 77.99   \u001b[0m | \u001b[0m 0.8016  \u001b[0m | \u001b[0m 11.52   \u001b[0m | \u001b[0m 9.111   \u001b[0m | \u001b[0m 1.92    \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 65.45   \u001b[0m | \u001b[0m 0.8528  \u001b[0m | \u001b[0m 0.03569 \u001b[0m | \u001b[0m 3.17    \u001b[0m | \u001b[0m 32.16   \u001b[0m | \u001b[0m 0.844   \u001b[0m | \u001b[0m 7.143   \u001b[0m | \u001b[0m 7.302   \u001b[0m | \u001b[0m 7.231   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 76.42   \u001b[0m | \u001b[0m 0.8674  \u001b[0m | \u001b[0m 0.2836  \u001b[0m | \u001b[0m 6.515   \u001b[0m | \u001b[0m 97.26   \u001b[0m | \u001b[0m 0.7244  \u001b[0m | \u001b[0m 1.213   \u001b[0m | \u001b[0m 10.13   \u001b[0m | \u001b[0m 2.724   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 73.23   \u001b[0m | \u001b[0m 0.963   \u001b[0m | \u001b[0m 0.3901  \u001b[0m | \u001b[0m 8.181   \u001b[0m | \u001b[0m 77.24   \u001b[0m | \u001b[0m 0.7225  \u001b[0m | \u001b[0m 2.556   \u001b[0m | \u001b[0m 1.683   \u001b[0m | \u001b[0m 1.898   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 77.71   \u001b[0m | \u001b[0m 0.8562  \u001b[0m | \u001b[0m 0.1153  \u001b[0m | \u001b[0m 4.354   \u001b[0m | \u001b[0m 74.17   \u001b[0m | \u001b[0m 0.7239  \u001b[0m | \u001b[0m 1.143   \u001b[0m | \u001b[0m 1.821   \u001b[0m | \u001b[0m 8.158   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 79.25   \u001b[0m | \u001b[0m 0.9295  \u001b[0m | \u001b[0m 0.2092  \u001b[0m | \u001b[0m 4.514   \u001b[0m | \u001b[0m 76.94   \u001b[0m | \u001b[0m 0.5782  \u001b[0m | \u001b[0m 7.627   \u001b[0m | \u001b[0m 12.06   \u001b[0m | \u001b[0m 7.267   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 76.02   \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 0.7468  \u001b[0m | \u001b[0m 5.903   \u001b[0m | \u001b[0m 72.51   \u001b[0m | \u001b[0m 0.6105  \u001b[0m | \u001b[0m 5.933   \u001b[0m | \u001b[0m 9.533   \u001b[0m | \u001b[0m 4.04    \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 79.89   \u001b[0m | \u001b[0m 0.9628  \u001b[0m | \u001b[0m 0.1588  \u001b[0m | \u001b[0m 4.942   \u001b[0m | \u001b[0m 48.9    \u001b[0m | \u001b[0m 0.8837  \u001b[0m | \u001b[0m 8.38    \u001b[0m | \u001b[0m 7.201   \u001b[0m | \u001b[0m 8.012   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 80.9    \u001b[0m | \u001b[0m 0.8896  \u001b[0m | \u001b[0m 0.83    \u001b[0m | \u001b[0m 3.115   \u001b[0m | \u001b[0m 28.06   \u001b[0m | \u001b[0m 0.6388  \u001b[0m | \u001b[0m 1.624   \u001b[0m | \u001b[0m 7.05    \u001b[0m | \u001b[0m 4.03    \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 73.42   \u001b[0m | \u001b[0m 0.9012  \u001b[0m | \u001b[0m 0.4698  \u001b[0m | \u001b[0m 6.224   \u001b[0m | \u001b[0m 33.88   \u001b[0m | \u001b[0m 0.8979  \u001b[0m | \u001b[0m 3.552   \u001b[0m | \u001b[0m 3.871   \u001b[0m | \u001b[0m 7.615   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 79.66   \u001b[0m | \u001b[0m 0.868   \u001b[0m | \u001b[0m 0.03041 \u001b[0m | \u001b[0m 8.323   \u001b[0m | \u001b[0m 81.95   \u001b[0m | \u001b[0m 0.8211  \u001b[0m | \u001b[0m 6.373   \u001b[0m | \u001b[0m 10.42   \u001b[0m | \u001b[0m 5.444   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 77.75   \u001b[0m | \u001b[0m 0.9836  \u001b[0m | \u001b[0m 0.5078  \u001b[0m | \u001b[0m 4.976   \u001b[0m | \u001b[0m 58.39   \u001b[0m | \u001b[0m 0.7365  \u001b[0m | \u001b[0m 7.3     \u001b[0m | \u001b[0m 8.247   \u001b[0m | \u001b[0m 7.515   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 73.43   \u001b[0m | \u001b[0m 0.8628  \u001b[0m | \u001b[0m 0.4668  \u001b[0m | \u001b[0m 3.602   \u001b[0m | \u001b[0m 73.54   \u001b[0m | \u001b[0m 0.7057  \u001b[0m | \u001b[0m 6.554   \u001b[0m | \u001b[0m 12.19   \u001b[0m | \u001b[0m 2.466   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 73.81   \u001b[0m | \u001b[0m 0.8615  \u001b[0m | \u001b[0m 0.2546  \u001b[0m | \u001b[0m 4.331   \u001b[0m | \u001b[0m 88.29   \u001b[0m | \u001b[0m 0.5914  \u001b[0m | \u001b[0m 5.739   \u001b[0m | \u001b[0m 5.331   \u001b[0m | \u001b[0m 8.028   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 63.35   \u001b[0m | \u001b[0m 0.8838  \u001b[0m | \u001b[0m 0.8334  \u001b[0m | \u001b[0m 8.111   \u001b[0m | \u001b[0m 62.87   \u001b[0m | \u001b[0m 0.5793  \u001b[0m | \u001b[0m 2.265   \u001b[0m | \u001b[0m 9.211   \u001b[0m | \u001b[0m 5.6     \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 76.14   \u001b[0m | \u001b[0m 0.8926  \u001b[0m | \u001b[0m 0.5041  \u001b[0m | \u001b[0m 3.015   \u001b[0m | \u001b[0m 85.8    \u001b[0m | \u001b[0m 0.7891  \u001b[0m | \u001b[0m 3.64    \u001b[0m | \u001b[0m 1.469   \u001b[0m | \u001b[0m 6.651   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 75.74   \u001b[0m | \u001b[0m 0.8712  \u001b[0m | \u001b[0m 0.3919  \u001b[0m | \u001b[0m 7.373   \u001b[0m | \u001b[0m 65.09   \u001b[0m | \u001b[0m 0.8923  \u001b[0m | \u001b[0m 3.289   \u001b[0m | \u001b[0m 7.209   \u001b[0m | \u001b[0m 1.857   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 74.87   \u001b[0m | \u001b[0m 0.9405  \u001b[0m | \u001b[0m 0.8491  \u001b[0m | \u001b[0m 5.122   \u001b[0m | \u001b[0m 30.27   \u001b[0m | \u001b[0m 0.6925  \u001b[0m | \u001b[0m 6.215   \u001b[0m | \u001b[0m 11.42   \u001b[0m | \u001b[0m 4.049   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 79.09   \u001b[0m | \u001b[0m 0.8951  \u001b[0m | \u001b[0m 0.1588  \u001b[0m | \u001b[0m 5.439   \u001b[0m | \u001b[0m 75.01   \u001b[0m | \u001b[0m 0.5176  \u001b[0m | \u001b[0m 4.551   \u001b[0m | \u001b[0m 2.681   \u001b[0m | \u001b[0m 1.206   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 80.81   \u001b[0m | \u001b[0m 0.9577  \u001b[0m | \u001b[0m 0.05175 \u001b[0m | \u001b[0m 4.724   \u001b[0m | \u001b[0m 58.1    \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m 7.336   \u001b[0m | \u001b[0m 7.593   \u001b[0m | \u001b[0m 7.52    \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 41.95   \u001b[0m | \u001b[0m 0.909   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 4.249   \u001b[0m | \u001b[0m 57.55   \u001b[0m | \u001b[0m 0.8078  \u001b[0m | \u001b[0m 7.405   \u001b[0m | \u001b[0m 6.361   \u001b[0m | \u001b[0m 7.528   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 63.82   \u001b[0m | \u001b[0m 0.9246  \u001b[0m | \u001b[0m 0.6571  \u001b[0m | \u001b[0m 6.642   \u001b[0m | \u001b[0m 72.66   \u001b[0m | \u001b[0m 0.7549  \u001b[0m | \u001b[0m 5.986   \u001b[0m | \u001b[0m 9.291   \u001b[0m | \u001b[0m 4.46    \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 71.96   \u001b[0m | \u001b[0m 0.945   \u001b[0m | \u001b[0m 0.7365  \u001b[0m | \u001b[0m 8.236   \u001b[0m | \u001b[0m 66.62   \u001b[0m | \u001b[0m 0.5378  \u001b[0m | \u001b[0m 5.138   \u001b[0m | \u001b[0m 8.268   \u001b[0m | \u001b[0m 1.378   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 59.02   \u001b[0m | \u001b[0m 0.9824  \u001b[0m | \u001b[0m 0.9775  \u001b[0m | \u001b[0m 6.754   \u001b[0m | \u001b[0m 31.36   \u001b[0m | \u001b[0m 0.8457  \u001b[0m | \u001b[0m 8.017   \u001b[0m | \u001b[0m 8.657   \u001b[0m | \u001b[0m 1.451   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 74.03   \u001b[0m | \u001b[0m 0.9291  \u001b[0m | \u001b[0m 0.8025  \u001b[0m | \u001b[0m 8.843   \u001b[0m | \u001b[0m 18.1    \u001b[0m | \u001b[0m 0.6886  \u001b[0m | \u001b[0m 4.501   \u001b[0m | \u001b[0m 2.267   \u001b[0m | \u001b[0m 6.535   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 71.73   \u001b[0m | \u001b[0m 0.9639  \u001b[0m | \u001b[0m 0.9038  \u001b[0m | \u001b[0m 7.015   \u001b[0m | \u001b[0m 66.07   \u001b[0m | \u001b[0m 0.6889  \u001b[0m | \u001b[0m 10.25   \u001b[0m | \u001b[0m 7.212   \u001b[0m | \u001b[0m 7.692   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 80.71   \u001b[0m | \u001b[0m 0.9669  \u001b[0m | \u001b[0m 0.8373  \u001b[0m | \u001b[0m 6.957   \u001b[0m | \u001b[0m 76.14   \u001b[0m | \u001b[0m 0.6534  \u001b[0m | \u001b[0m 8.617   \u001b[0m | \u001b[0m 4.857   \u001b[0m | \u001b[0m 1.514   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 78.26   \u001b[0m | \u001b[0m 0.9576  \u001b[0m | \u001b[0m 0.2038  \u001b[0m | \u001b[0m 7.571   \u001b[0m | \u001b[0m 97.36   \u001b[0m | \u001b[0m 0.7108  \u001b[0m | \u001b[0m 5.449   \u001b[0m | \u001b[0m 3.416   \u001b[0m | \u001b[0m 6.017   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 78.29   \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 0.3012  \u001b[0m | \u001b[0m 7.755   \u001b[0m | \u001b[0m 70.49   \u001b[0m | \u001b[0m 0.6172  \u001b[0m | \u001b[0m 1.476   \u001b[0m | \u001b[0m 8.281   \u001b[0m | \u001b[0m 2.771   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 64.65   \u001b[0m | \u001b[0m 0.8935  \u001b[0m | \u001b[0m 0.9137  \u001b[0m | \u001b[0m 7.51    \u001b[0m | \u001b[0m 51.1    \u001b[0m | \u001b[0m 0.823   \u001b[0m | \u001b[0m 7.375   \u001b[0m | \u001b[0m 5.611   \u001b[0m | \u001b[0m 6.214   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 75.05   \u001b[0m | \u001b[0m 0.9424  \u001b[0m | \u001b[0m 0.04905 \u001b[0m | \u001b[0m 4.363   \u001b[0m | \u001b[0m 38.34   \u001b[0m | \u001b[0m 0.7753  \u001b[0m | \u001b[0m 6.395   \u001b[0m | \u001b[0m 1.156   \u001b[0m | \u001b[0m 2.845   \u001b[0m |\n",
      "| \u001b[95m 46      \u001b[0m | \u001b[95m 83.53   \u001b[0m | \u001b[95m 0.9747  \u001b[0m | \u001b[95m 0.8052  \u001b[0m | \u001b[95m 7.615   \u001b[0m | \u001b[95m 88.74   \u001b[0m | \u001b[95m 0.8257  \u001b[0m | \u001b[95m 8.995   \u001b[0m | \u001b[95m 5.721   \u001b[0m | \u001b[95m 2.719   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 71.75   \u001b[0m | \u001b[0m 0.8694  \u001b[0m | \u001b[0m 0.3213  \u001b[0m | \u001b[0m 7.759   \u001b[0m | \u001b[0m 91.61   \u001b[0m | \u001b[0m 0.8793  \u001b[0m | \u001b[0m 3.927   \u001b[0m | \u001b[0m 7.919   \u001b[0m | \u001b[0m 7.577   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 79.18   \u001b[0m | \u001b[0m 0.9151  \u001b[0m | \u001b[0m 0.9626  \u001b[0m | \u001b[0m 4.606   \u001b[0m | \u001b[0m 85.02   \u001b[0m | \u001b[0m 0.7352  \u001b[0m | \u001b[0m 6.533   \u001b[0m | \u001b[0m 3.093   \u001b[0m | \u001b[0m 3.949   \u001b[0m |\n",
      "| \u001b[95m 49      \u001b[0m | \u001b[95m 83.84   \u001b[0m | \u001b[95m 0.9658  \u001b[0m | \u001b[95m 0.131   \u001b[0m | \u001b[95m 5.432   \u001b[0m | \u001b[95m 41.9    \u001b[0m | \u001b[95m 0.7071  \u001b[0m | \u001b[95m 12.32   \u001b[0m | \u001b[95m 3.096   \u001b[0m | \u001b[95m 4.414   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 69.77   \u001b[0m | \u001b[0m 0.9252  \u001b[0m | \u001b[0m 0.9597  \u001b[0m | \u001b[0m 7.661   \u001b[0m | \u001b[0m 54.86   \u001b[0m | \u001b[0m 0.534   \u001b[0m | \u001b[0m 12.24   \u001b[0m | \u001b[0m 2.498   \u001b[0m | \u001b[0m 3.14    \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 76.95   \u001b[0m | \u001b[0m 0.8694  \u001b[0m | \u001b[0m 0.182   \u001b[0m | \u001b[0m 3.777   \u001b[0m | \u001b[0m 78.78   \u001b[0m | \u001b[0m 0.5031  \u001b[0m | \u001b[0m 6.385   \u001b[0m | \u001b[0m 5.757   \u001b[0m | \u001b[0m 3.038   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 81.91   \u001b[0m | \u001b[0m 0.9758  \u001b[0m | \u001b[0m 0.2867  \u001b[0m | \u001b[0m 4.959   \u001b[0m | \u001b[0m 90.91   \u001b[0m | \u001b[0m 0.7904  \u001b[0m | \u001b[0m 10.42   \u001b[0m | \u001b[0m 6.422   \u001b[0m | \u001b[0m 1.764   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 71.46   \u001b[0m | \u001b[0m 0.912   \u001b[0m | \u001b[0m 0.815   \u001b[0m | \u001b[0m 5.05    \u001b[0m | \u001b[0m 71.88   \u001b[0m | \u001b[0m 0.8688  \u001b[0m | \u001b[0m 11.88   \u001b[0m | \u001b[0m 11.73   \u001b[0m | \u001b[0m 4.472   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 77.15   \u001b[0m | \u001b[0m 0.9474  \u001b[0m | \u001b[0m 0.8018  \u001b[0m | \u001b[0m 3.052   \u001b[0m | \u001b[0m 98.28   \u001b[0m | \u001b[0m 0.6342  \u001b[0m | \u001b[0m 2.469   \u001b[0m | \u001b[0m 4.545   \u001b[0m | \u001b[0m 3.044   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 80.59   \u001b[0m | \u001b[0m 0.9155  \u001b[0m | \u001b[0m 0.1029  \u001b[0m | \u001b[0m 4.474   \u001b[0m | \u001b[0m 37.38   \u001b[0m | \u001b[0m 0.6649  \u001b[0m | \u001b[0m 4.023   \u001b[0m | \u001b[0m 3.313   \u001b[0m | \u001b[0m 5.993   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 73.06   \u001b[0m | \u001b[0m 0.9288  \u001b[0m | \u001b[0m 0.4347  \u001b[0m | \u001b[0m 6.406   \u001b[0m | \u001b[0m 19.45   \u001b[0m | \u001b[0m 0.7834  \u001b[0m | \u001b[0m 2.638   \u001b[0m | \u001b[0m 9.4     \u001b[0m | \u001b[0m 5.946   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 69.66   \u001b[0m | \u001b[0m 0.9758  \u001b[0m | \u001b[0m 0.3067  \u001b[0m | \u001b[0m 4.515   \u001b[0m | \u001b[0m 29.88   \u001b[0m | \u001b[0m 0.506   \u001b[0m | \u001b[0m 5.874   \u001b[0m | \u001b[0m 5.686   \u001b[0m | \u001b[0m 5.77    \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 80.75   \u001b[0m | \u001b[0m 0.8751  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 3.164   \u001b[0m | \u001b[0m 74.64   \u001b[0m | \u001b[0m 0.8394  \u001b[0m | \u001b[0m 10.41   \u001b[0m | \u001b[0m 4.115   \u001b[0m | \u001b[0m 2.029   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 78.14   \u001b[0m | \u001b[0m 0.934   \u001b[0m | \u001b[0m 0.3095  \u001b[0m | \u001b[0m 3.692   \u001b[0m | \u001b[0m 11.89   \u001b[0m | \u001b[0m 0.5097  \u001b[0m | \u001b[0m 11.04   \u001b[0m | \u001b[0m 1.611   \u001b[0m | \u001b[0m 7.484   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 78.08   \u001b[0m | \u001b[0m 0.9187  \u001b[0m | \u001b[0m 0.1158  \u001b[0m | \u001b[0m 3.612   \u001b[0m | \u001b[0m 59.19   \u001b[0m | \u001b[0m 0.6884  \u001b[0m | \u001b[0m 6.397   \u001b[0m | \u001b[0m 9.998   \u001b[0m | \u001b[0m 1.692   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 80.36   \u001b[0m | \u001b[0m 0.9603  \u001b[0m | \u001b[0m 0.2775  \u001b[0m | \u001b[0m 5.3     \u001b[0m | \u001b[0m 43.99   \u001b[0m | \u001b[0m 0.6517  \u001b[0m | \u001b[0m 5.522   \u001b[0m | \u001b[0m 3.345   \u001b[0m | \u001b[0m 2.381   \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 75.55   \u001b[0m | \u001b[0m 0.8969  \u001b[0m | \u001b[0m 0.6719  \u001b[0m | \u001b[0m 6.21    \u001b[0m | \u001b[0m 17.18   \u001b[0m | \u001b[0m 0.6473  \u001b[0m | \u001b[0m 8.753   \u001b[0m | \u001b[0m 5.468   \u001b[0m | \u001b[0m 5.922   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 79.72   \u001b[0m | \u001b[0m 0.9565  \u001b[0m | \u001b[0m 0.08124 \u001b[0m | \u001b[0m 7.334   \u001b[0m | \u001b[0m 24.24   \u001b[0m | \u001b[0m 0.7349  \u001b[0m | \u001b[0m 5.031   \u001b[0m | \u001b[0m 9.54    \u001b[0m | \u001b[0m 2.051   \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 72.5    \u001b[0m | \u001b[0m 0.8813  \u001b[0m | \u001b[0m 0.0288  \u001b[0m | \u001b[0m 3.296   \u001b[0m | \u001b[0m 50.32   \u001b[0m | \u001b[0m 0.5693  \u001b[0m | \u001b[0m 4.496   \u001b[0m | \u001b[0m 11.26   \u001b[0m | \u001b[0m 2.545   \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 72.66   \u001b[0m | \u001b[0m 0.9268  \u001b[0m | \u001b[0m 0.06042 \u001b[0m | \u001b[0m 6.205   \u001b[0m | \u001b[0m 24.11   \u001b[0m | \u001b[0m 0.6784  \u001b[0m | \u001b[0m 3.994   \u001b[0m | \u001b[0m 7.688   \u001b[0m | \u001b[0m 5.066   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 77.47   \u001b[0m | \u001b[0m 0.9706  \u001b[0m | \u001b[0m 0.5763  \u001b[0m | \u001b[0m 6.566   \u001b[0m | \u001b[0m 17.9    \u001b[0m | \u001b[0m 0.5164  \u001b[0m | \u001b[0m 1.555   \u001b[0m | \u001b[0m 11.45   \u001b[0m | \u001b[0m 1.66    \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 81.45   \u001b[0m | \u001b[0m 0.9182  \u001b[0m | \u001b[0m 0.1571  \u001b[0m | \u001b[0m 4.833   \u001b[0m | \u001b[0m 93.7    \u001b[0m | \u001b[0m 0.5227  \u001b[0m | \u001b[0m 11.1    \u001b[0m | \u001b[0m 1.999   \u001b[0m | \u001b[0m 8.226   \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 74.66   \u001b[0m | \u001b[0m 0.8635  \u001b[0m | \u001b[0m 0.713   \u001b[0m | \u001b[0m 4.048   \u001b[0m | \u001b[0m 73.76   \u001b[0m | \u001b[0m 0.59    \u001b[0m | \u001b[0m 5.684   \u001b[0m | \u001b[0m 6.903   \u001b[0m | \u001b[0m 5.769   \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 68.81   \u001b[0m | \u001b[0m 0.9008  \u001b[0m | \u001b[0m 0.7161  \u001b[0m | \u001b[0m 7.748   \u001b[0m | \u001b[0m 51.4    \u001b[0m | \u001b[0m 0.611   \u001b[0m | \u001b[0m 9.946   \u001b[0m | \u001b[0m 1.3     \u001b[0m | \u001b[0m 5.969   \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 81.57   \u001b[0m | \u001b[0m 0.885   \u001b[0m | \u001b[0m 0.1957  \u001b[0m | \u001b[0m 4.469   \u001b[0m | \u001b[0m 31.31   \u001b[0m | \u001b[0m 0.6938  \u001b[0m | \u001b[0m 3.204   \u001b[0m | \u001b[0m 5.426   \u001b[0m | \u001b[0m 3.615   \u001b[0m |\n",
      "| \u001b[95m 71      \u001b[0m | \u001b[95m 84.13   \u001b[0m | \u001b[95m 0.9039  \u001b[0m | \u001b[95m 0.2152  \u001b[0m | \u001b[95m 3.37    \u001b[0m | \u001b[95m 23.41   \u001b[0m | \u001b[95m 0.8991  \u001b[0m | \u001b[95m 9.166   \u001b[0m | \u001b[95m 2.117   \u001b[0m | \u001b[95m 4.046   \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 78.81   \u001b[0m | \u001b[0m 0.8983  \u001b[0m | \u001b[0m 0.5465  \u001b[0m | \u001b[0m 7.101   \u001b[0m | \u001b[0m 11.6    \u001b[0m | \u001b[0m 0.5371  \u001b[0m | \u001b[0m 7.619   \u001b[0m | \u001b[0m 2.635   \u001b[0m | \u001b[0m 8.025   \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 75.11   \u001b[0m | \u001b[0m 0.8852  \u001b[0m | \u001b[0m 0.2932  \u001b[0m | \u001b[0m 4.631   \u001b[0m | \u001b[0m 55.09   \u001b[0m | \u001b[0m 0.7035  \u001b[0m | \u001b[0m 1.787   \u001b[0m | \u001b[0m 4.715   \u001b[0m | \u001b[0m 5.094   \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 72.27   \u001b[0m | \u001b[0m 0.8781  \u001b[0m | \u001b[0m 0.5555  \u001b[0m | \u001b[0m 6.737   \u001b[0m | \u001b[0m 58.5    \u001b[0m | \u001b[0m 0.8409  \u001b[0m | \u001b[0m 6.558   \u001b[0m | \u001b[0m 10.82   \u001b[0m | \u001b[0m 5.603   \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 70.76   \u001b[0m | \u001b[0m 0.8845  \u001b[0m | \u001b[0m 0.6104  \u001b[0m | \u001b[0m 4.037   \u001b[0m | \u001b[0m 64.94   \u001b[0m | \u001b[0m 0.7225  \u001b[0m | \u001b[0m 4.118   \u001b[0m | \u001b[0m 2.062   \u001b[0m | \u001b[0m 7.111   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 81.28   \u001b[0m | \u001b[0m 0.9479  \u001b[0m | \u001b[0m 0.6662  \u001b[0m | \u001b[0m 6.512   \u001b[0m | \u001b[0m 78.3    \u001b[0m | \u001b[0m 0.6566  \u001b[0m | \u001b[0m 6.747   \u001b[0m | \u001b[0m 6.639   \u001b[0m | \u001b[0m 2.198   \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 82.14   \u001b[0m | \u001b[0m 0.9686  \u001b[0m | \u001b[0m 0.1885  \u001b[0m | \u001b[0m 8.904   \u001b[0m | \u001b[0m 50.99   \u001b[0m | \u001b[0m 0.8059  \u001b[0m | \u001b[0m 9.244   \u001b[0m | \u001b[0m 4.257   \u001b[0m | \u001b[0m 1.601   \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 75.05   \u001b[0m | \u001b[0m 0.969   \u001b[0m | \u001b[0m 0.8989  \u001b[0m | \u001b[0m 8.9     \u001b[0m | \u001b[0m 63.53   \u001b[0m | \u001b[0m 0.6525  \u001b[0m | \u001b[0m 8.564   \u001b[0m | \u001b[0m 2.97    \u001b[0m | \u001b[0m 4.545   \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 79.44   \u001b[0m | \u001b[0m 0.9736  \u001b[0m | \u001b[0m 0.4638  \u001b[0m | \u001b[0m 4.922   \u001b[0m | \u001b[0m 70.87   \u001b[0m | \u001b[0m 0.6259  \u001b[0m | \u001b[0m 10.07   \u001b[0m | \u001b[0m 9.228   \u001b[0m | \u001b[0m 7.394   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 71.77   \u001b[0m | \u001b[0m 0.8675  \u001b[0m | \u001b[0m 0.5566  \u001b[0m | \u001b[0m 4.653   \u001b[0m | \u001b[0m 96.02   \u001b[0m | \u001b[0m 0.8985  \u001b[0m | \u001b[0m 3.064   \u001b[0m | \u001b[0m 5.225   \u001b[0m | \u001b[0m 4.136   \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 78.69   \u001b[0m | \u001b[0m 0.9568  \u001b[0m | \u001b[0m 0.6234  \u001b[0m | \u001b[0m 8.662   \u001b[0m | \u001b[0m 27.42   \u001b[0m | \u001b[0m 0.6123  \u001b[0m | \u001b[0m 5.859   \u001b[0m | \u001b[0m 6.135   \u001b[0m | \u001b[0m 7.103   \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 78.4    \u001b[0m | \u001b[0m 0.9294  \u001b[0m | \u001b[0m 0.7252  \u001b[0m | \u001b[0m 4.666   \u001b[0m | \u001b[0m 12.8    \u001b[0m | \u001b[0m 0.7776  \u001b[0m | \u001b[0m 11.43   \u001b[0m | \u001b[0m 11.28   \u001b[0m | \u001b[0m 3.278   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 79.52   \u001b[0m | \u001b[0m 0.9307  \u001b[0m | \u001b[0m 0.8089  \u001b[0m | \u001b[0m 5.361   \u001b[0m | \u001b[0m 64.34   \u001b[0m | \u001b[0m 0.6365  \u001b[0m | \u001b[0m 9.186   \u001b[0m | \u001b[0m 3.817   \u001b[0m | \u001b[0m 2.246   \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 79.49   \u001b[0m | \u001b[0m 0.8975  \u001b[0m | \u001b[0m 0.7784  \u001b[0m | \u001b[0m 4.247   \u001b[0m | \u001b[0m 36.44   \u001b[0m | \u001b[0m 0.878   \u001b[0m | \u001b[0m 5.793   \u001b[0m | \u001b[0m 10.81   \u001b[0m | \u001b[0m 5.365   \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 77.55   \u001b[0m | \u001b[0m 0.9275  \u001b[0m | \u001b[0m 0.6411  \u001b[0m | \u001b[0m 4.239   \u001b[0m | \u001b[0m 28.28   \u001b[0m | \u001b[0m 0.7051  \u001b[0m | \u001b[0m 3.692   \u001b[0m | \u001b[0m 6.241   \u001b[0m | \u001b[0m 4.155   \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 78.5    \u001b[0m | \u001b[0m 0.9153  \u001b[0m | \u001b[0m 0.4527  \u001b[0m | \u001b[0m 8.158   \u001b[0m | \u001b[0m 38.13   \u001b[0m | \u001b[0m 0.627   \u001b[0m | \u001b[0m 6.024   \u001b[0m | \u001b[0m 10.07   \u001b[0m | \u001b[0m 1.716   \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 83.45   \u001b[0m | \u001b[0m 0.9004  \u001b[0m | \u001b[0m 0.1734  \u001b[0m | \u001b[0m 8.34    \u001b[0m | \u001b[0m 15.8    \u001b[0m | \u001b[0m 0.6853  \u001b[0m | \u001b[0m 9.357   \u001b[0m | \u001b[0m 9.78    \u001b[0m | \u001b[0m 5.713   \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 77.45   \u001b[0m | \u001b[0m 0.8836  \u001b[0m | \u001b[0m 0.5624  \u001b[0m | \u001b[0m 8.587   \u001b[0m | \u001b[0m 53.46   \u001b[0m | \u001b[0m 0.6877  \u001b[0m | \u001b[0m 12.38   \u001b[0m | \u001b[0m 4.371   \u001b[0m | \u001b[0m 5.721   \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 77.6    \u001b[0m | \u001b[0m 0.8709  \u001b[0m | \u001b[0m 0.4488  \u001b[0m | \u001b[0m 5.66    \u001b[0m | \u001b[0m 96.74   \u001b[0m | \u001b[0m 0.5083  \u001b[0m | \u001b[0m 1.248   \u001b[0m | \u001b[0m 8.012   \u001b[0m | \u001b[0m 2.601   \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 70.9    \u001b[0m | \u001b[0m 0.9791  \u001b[0m | \u001b[0m 0.8276  \u001b[0m | \u001b[0m 6.656   \u001b[0m | \u001b[0m 81.32   \u001b[0m | \u001b[0m 0.5548  \u001b[0m | \u001b[0m 9.452   \u001b[0m | \u001b[0m 8.716   \u001b[0m | \u001b[0m 3.984   \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 79.92   \u001b[0m | \u001b[0m 0.9101  \u001b[0m | \u001b[0m 0.07889 \u001b[0m | \u001b[0m 3.333   \u001b[0m | \u001b[0m 43.49   \u001b[0m | \u001b[0m 0.8969  \u001b[0m | \u001b[0m 1.866   \u001b[0m | \u001b[0m 10.48   \u001b[0m | \u001b[0m 6.607   \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 84.06   \u001b[0m | \u001b[0m 0.9546  \u001b[0m | \u001b[0m 0.2958  \u001b[0m | \u001b[0m 8.947   \u001b[0m | \u001b[0m 37.63   \u001b[0m | \u001b[0m 0.6305  \u001b[0m | \u001b[0m 12.09   \u001b[0m | \u001b[0m 9.789   \u001b[0m | \u001b[0m 5.297   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 61.47   \u001b[0m | \u001b[0m 0.8625  \u001b[0m | \u001b[0m 0.9666  \u001b[0m | \u001b[0m 5.055   \u001b[0m | \u001b[0m 95.16   \u001b[0m | \u001b[0m 0.6834  \u001b[0m | \u001b[0m 1.828   \u001b[0m | \u001b[0m 11.18   \u001b[0m | \u001b[0m 4.005   \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 73.28   \u001b[0m | \u001b[0m 0.8895  \u001b[0m | \u001b[0m 0.231   \u001b[0m | \u001b[0m 4.329   \u001b[0m | \u001b[0m 66.36   \u001b[0m | \u001b[0m 0.5932  \u001b[0m | \u001b[0m 6.146   \u001b[0m | \u001b[0m 11.36   \u001b[0m | \u001b[0m 4.958   \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 78.87   \u001b[0m | \u001b[0m 0.8998  \u001b[0m | \u001b[0m 0.2935  \u001b[0m | \u001b[0m 3.681   \u001b[0m | \u001b[0m 48.39   \u001b[0m | \u001b[0m 0.839   \u001b[0m | \u001b[0m 2.864   \u001b[0m | \u001b[0m 10.86   \u001b[0m | \u001b[0m 4.496   \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 82.74   \u001b[0m | \u001b[0m 0.9127  \u001b[0m | \u001b[0m 0.4725  \u001b[0m | \u001b[0m 5.974   \u001b[0m | \u001b[0m 97.53   \u001b[0m | \u001b[0m 0.8615  \u001b[0m | \u001b[0m 6.589   \u001b[0m | \u001b[0m 11.6    \u001b[0m | \u001b[0m 3.058   \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 74.4    \u001b[0m | \u001b[0m 0.9601  \u001b[0m | \u001b[0m 0.5162  \u001b[0m | \u001b[0m 4.594   \u001b[0m | \u001b[0m 27.34   \u001b[0m | \u001b[0m 0.8796  \u001b[0m | \u001b[0m 6.929   \u001b[0m | \u001b[0m 5.871   \u001b[0m | \u001b[0m 4.316   \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 82.89   \u001b[0m | \u001b[0m 0.9476  \u001b[0m | \u001b[0m 0.8998  \u001b[0m | \u001b[0m 4.606   \u001b[0m | \u001b[0m 11.42   \u001b[0m | \u001b[0m 0.6551  \u001b[0m | \u001b[0m 4.359   \u001b[0m | \u001b[0m 5.867   \u001b[0m | \u001b[0m 7.096   \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 64.23   \u001b[0m | \u001b[0m 0.8805  \u001b[0m | \u001b[0m 0.6963  \u001b[0m | \u001b[0m 7.938   \u001b[0m | \u001b[0m 64.33   \u001b[0m | \u001b[0m 0.8961  \u001b[0m | \u001b[0m 7.889   \u001b[0m | \u001b[0m 3.608   \u001b[0m | \u001b[0m 3.945   \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 77.93   \u001b[0m | \u001b[0m 0.8594  \u001b[0m | \u001b[0m 0.3752  \u001b[0m | \u001b[0m 7.993   \u001b[0m | \u001b[0m 41.7    \u001b[0m | \u001b[0m 0.8998  \u001b[0m | \u001b[0m 7.055   \u001b[0m | \u001b[0m 11.62   \u001b[0m | \u001b[0m 4.415   \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "param_bound = {'alpha' : (0.85,0.99) , 'm_n_estimator' : (10, 100), 'm_lr' : (0.01, 0.99), 'm_subsample' : (0.5, 0.9), 'm_max_depth' : (3,9), 't_col_k' : (1,8.4), 's_col_k' : (1,12.4), 'p_col_k' : (1,12.4)}\n",
    "def h_opt(alpha, m_n_estimator, m_lr, m_subsample, m_max_depth, t_col_k, s_col_k, p_col_k):     \n",
    "    weight_mat = list(map(lambda x : round(alpha,3)**x if x > 0 else 1, range(0,12)))    \n",
    "    model_=xgboost.XGBRegressor(n_estimators=round(m_n_estimator), learning_rate=round(m_lr,3), subsample=round(m_subsample,3), max_depth=round(m_max_depth), tree_method='gpu_hist', gpu_id=0)\n",
    "    #model_= Ridge()#LR(m_lr) #\n",
    "    \n",
    "    acc_, acc_hat_, _, _, _, _  = run_model_1(model_, d_set1, weight_mat, round(t_col_k), round(s_col_k), round(p_col_k), 201903, 202043, 202044, 202051)# # prev :/ var : \n",
    "    return acc_hat_    \n",
    "\n",
    "sale_optimizer = BayesianOptimization(f=h_opt, pbounds=param_bound, verbose=2, random_state=1)\n",
    "sale_optimizer.maximize(init_points=20, n_iter=80)\n",
    "####################################################################################################################################\n",
    "#####    |  target   |   alpha   |   m_lr    | m_max_... | m_n_es... | m_subs... |  p_col_k  |  s_col_k  |  t_col_k  |\n",
    "#####    |  86.88    |  0.9218   |  0.5155   |  3.912    |  52.12    |  0.6188   |  9.2      |  6.628    |  5.863    |   => XGBoost\n",
    "#####    |  90.48    |  0.85     |  0.01     |  5.05     |  68.06    |  0.9      |  1.0      |  1.0      |  6.371    |   => KR\n",
    "#####    |  93.21    |  0.99     |  0.99     |  6.08     |  65.54    |  0.5      |  1.0      |  1.0      |  6.13     |   => LR\n",
    "#####    |  93.21    |  0.85     |  0.99     |  5.482    |  88.84    |  0.9      |  1.0      |  1.0      |  5.534    |   => Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d32b43bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |   col_k   |  col_k1   |   m_lr    | m_max_... | m_n_es... | m_subs... |\n",
      "-------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.9083830806583604, 9.643893921305898, 1.0013724978081386, 0.1551196348632831, 3.880535344902678, 18.310473529191803, 0.4931301056888355)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-08d3fd1e93ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mproduct_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mproduct_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#####################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#####    |  target   |   alpha   |   col_k   |  col_k1   |   m_lr    | m_max_... | m_n_es... | m_subs... |\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-08d3fd1e93ae>\u001b[0m in \u001b[0;36mh_opt\u001b[0;34m(alpha, m_n_estimator, m_lr, m_subsample, m_max_depth, col_k, col_k1)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#model_ = KR(m_lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#acc_, acc_hat_, _, _, _, _ = run_model_(model_, d_set, 'Product', round(col_k), 201903, 202043, 202044, 202053)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_hat_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_set1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201945\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202102\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202102\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202112\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_k1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prev / var :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc_hat_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-34b9fb672592>\u001b[0m in \u001b[0;36mrun_model_2\u001b[0;34m(m_, x_dta, target_key, wt_mat, tr_s_day, tr_e_day, te_s_day, te_e_day, col_k, col_k1)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mhat_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_y_hat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mY_hat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhat_prev_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#hat_))#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mY_\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_y_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_y_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfcst_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_y_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_y_set\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhat_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mte_y_set\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "param_bound = {'alpha' : (0.85,0.99) , 'm_n_estimator' : (10, 100), 'm_lr' : (0.01, 0.49), 'm_subsample' : (0.4, 0.9), 'm_max_depth' : (3,9), 'col_k' : (1,13), 'col_k1' : (1,13)}\n",
    "def h_opt(alpha, m_n_estimator, m_lr, m_subsample, m_max_depth, col_k, col_k1):\n",
    "    weight_mat = list(map(lambda x : round(alpha,3)**x if x > 0 else 1, range(0,12)))    #\n",
    "    model_=xgboost.XGBRegressor(n_estimators=round(m_n_estimator), learning_rate=round(m_lr,3), subsample=round(m_subsample,3), max_depth=round(m_max_depth), tree_method='gpu_hist', gpu_id=0)\n",
    "    #model_ = KR(m_lr)\n",
    "    #acc_, acc_hat_, _, _, _, _ = run_model_(model_, d_set, 'Product', round(col_k), 201903, 202043, 202044, 202053)\n",
    "    acc_, acc_hat_, _, _, _, _ = run_model_2(model_, d_set1, 'Sale', weight_mat, 201945, 202102, 202102, 202112, None, round(col_k1))  # prev / var : \n",
    "    return acc_hat_    \n",
    "\n",
    "product_optimizer = BayesianOptimization(f=h_opt, pbounds=param_bound, verbose=2, random_state=1)\n",
    "product_optimizer.maximize(init_points=10, n_iter=100)\n",
    "#####################################################################################################################\n",
    "#####    |  target   |   alpha   |   col_k   |  col_k1   |   m_lr    | m_max_... | m_n_es... | m_subs... |\n",
    "#####    |  69.22    |  0.9421   |  9.493    |  6.008    |  0.1001   |  4.943    |  43.33    |  0.5873   |   => XGBoost\n",
    "#####    |  78.24    |  0.9153   |  11.54    |  2.18     |  0.4227   |  8.747    |  57.98    |  0.7459   |  => Kernel_Ridge : Var_sale\n",
    "#####    |  62.31    |  0.8557   |  1.942    |  10.75    |  0.2537   |  7.427    |  24.23    |  0.5428   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb729f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1818ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |   col_k   |  col_k1   |   m_lr    | m_max_... | m_n_es... | m_subs... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 83.74   \u001b[0m | \u001b[0m 0.9084  \u001b[0m | \u001b[0m 9.644   \u001b[0m | \u001b[0m 1.001   \u001b[0m | \u001b[0m 0.1551  \u001b[0m | \u001b[0m 3.881   \u001b[0m | \u001b[0m 18.31   \u001b[0m | \u001b[0m 0.4931  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 84.15   \u001b[0m | \u001b[95m 0.8984  \u001b[0m | \u001b[95m 5.761   \u001b[0m | \u001b[95m 7.466   \u001b[0m | \u001b[95m 0.2112  \u001b[0m | \u001b[95m 7.111   \u001b[0m | \u001b[95m 28.4    \u001b[0m | \u001b[95m 0.8391  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 73.18   \u001b[0m | \u001b[0m 0.8538  \u001b[0m | \u001b[0m 9.046   \u001b[0m | \u001b[0m 6.008   \u001b[0m | \u001b[0m 0.2782  \u001b[0m | \u001b[0m 3.842   \u001b[0m | \u001b[0m 27.83   \u001b[0m | \u001b[0m 0.8004  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 66.53   \u001b[0m | \u001b[0m 0.9856  \u001b[0m | \u001b[0m 4.761   \u001b[0m | \u001b[0m 9.308   \u001b[0m | \u001b[0m 0.4307  \u001b[0m | \u001b[0m 8.368   \u001b[0m | \u001b[0m 17.65   \u001b[0m | \u001b[0m 0.4195  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 71.26   \u001b[0m | \u001b[0m 0.8738  \u001b[0m | \u001b[0m 11.54   \u001b[0m | \u001b[0m 2.18    \u001b[0m | \u001b[0m 0.2121  \u001b[0m | \u001b[0m 8.747   \u001b[0m | \u001b[0m 57.98   \u001b[0m | \u001b[0m 0.7459  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 84.83   \u001b[0m | \u001b[95m 0.8942  \u001b[0m | \u001b[95m 9.238   \u001b[0m | \u001b[95m 11.02   \u001b[0m | \u001b[95m 0.01878 \u001b[0m | \u001b[95m 7.501   \u001b[0m | \u001b[95m 99.0    \u001b[0m | \u001b[95m 0.7741  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 71.15   \u001b[0m | \u001b[0m 0.8893  \u001b[0m | \u001b[0m 10.47   \u001b[0m | \u001b[0m 2.239   \u001b[0m | \u001b[0m 0.225   \u001b[0m | \u001b[0m 8.452   \u001b[0m | \u001b[0m 36.43   \u001b[0m | \u001b[0m 0.5439  \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 86.89   \u001b[0m | \u001b[95m 0.8682  \u001b[0m | \u001b[95m 1.232   \u001b[0m | \u001b[95m 9.146   \u001b[0m | \u001b[95m 0.1116  \u001b[0m | \u001b[95m 4.593   \u001b[0m | \u001b[95m 54.24   \u001b[0m | \u001b[95m 0.4267  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 84.74   \u001b[0m | \u001b[0m 0.9304  \u001b[0m | \u001b[0m 2.761   \u001b[0m | \u001b[0m 8.072   \u001b[0m | \u001b[0m 0.3459  \u001b[0m | \u001b[0m 3.614   \u001b[0m | \u001b[0m 47.27   \u001b[0m | \u001b[0m 0.7472  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 86.87   \u001b[0m | \u001b[0m 0.908   \u001b[0m | \u001b[0m 1.599   \u001b[0m | \u001b[0m 7.431   \u001b[0m | \u001b[0m 0.3286  \u001b[0m | \u001b[0m 6.089   \u001b[0m | \u001b[0m 95.01   \u001b[0m | \u001b[0m 0.6933  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 86.31   \u001b[0m | \u001b[0m 0.9228  \u001b[0m | \u001b[0m 5.239   \u001b[0m | \u001b[0m 7.543   \u001b[0m | \u001b[0m 0.4371  \u001b[0m | \u001b[0m 7.383   \u001b[0m | \u001b[0m 28.59   \u001b[0m | \u001b[0m 0.8001  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 86.82   \u001b[0m | \u001b[0m 0.9899  \u001b[0m | \u001b[0m 1.506   \u001b[0m | \u001b[0m 9.016   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 30.31   \u001b[0m | \u001b[0m 0.8609  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 81.74   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.256   \u001b[0m | \u001b[0m 3.949   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 28.5    \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 84.6    \u001b[0m | \u001b[0m 0.9109  \u001b[0m | \u001b[0m 2.431   \u001b[0m | \u001b[0m 10.35   \u001b[0m | \u001b[0m 0.2412  \u001b[0m | \u001b[0m 6.172   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.6883  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 74.61   \u001b[0m | \u001b[0m 0.8585  \u001b[0m | \u001b[0m 6.515   \u001b[0m | \u001b[0m 4.593   \u001b[0m | \u001b[0m 0.2517  \u001b[0m | \u001b[0m 5.745   \u001b[0m | \u001b[0m 97.22   \u001b[0m | \u001b[0m 0.6143  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 82.95   \u001b[0m | \u001b[0m 0.9474  \u001b[0m | \u001b[0m 1.877   \u001b[0m | \u001b[0m 12.13   \u001b[0m | \u001b[0m 0.2069  \u001b[0m | \u001b[0m 7.425   \u001b[0m | \u001b[0m 94.38   \u001b[0m | \u001b[0m 0.8321  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 86.13   \u001b[0m | \u001b[0m 0.9749  \u001b[0m | \u001b[0m 3.56    \u001b[0m | \u001b[0m 11.81   \u001b[0m | \u001b[0m 0.4235  \u001b[0m | \u001b[0m 7.532   \u001b[0m | \u001b[0m 28.89   \u001b[0m | \u001b[0m 0.4494  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 79.74   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 13.0    \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 51.0    \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 32.84   \u001b[0m | \u001b[0m 0.85    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 4.581   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 4.122   \u001b[0m | \u001b[0m 52.17   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 76.79   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 2.041   \u001b[0m | \u001b[0m 8.733   \u001b[0m | \u001b[0m 0.2928  \u001b[0m | \u001b[0m 7.274   \u001b[0m | \u001b[0m 27.27   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 76.96   \u001b[0m | \u001b[0m 0.9458  \u001b[0m | \u001b[0m 4.746   \u001b[0m | \u001b[0m 9.561   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 8.807   \u001b[0m | \u001b[0m 30.95   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 86.84   \u001b[0m | \u001b[0m 0.8748  \u001b[0m | \u001b[0m 1.412   \u001b[0m | \u001b[0m 11.59   \u001b[0m | \u001b[0m 0.1591  \u001b[0m | \u001b[0m 4.912   \u001b[0m | \u001b[0m 55.42   \u001b[0m | \u001b[0m 0.4287  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 85.08   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 9.078   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 8.007   \u001b[0m | \u001b[0m 97.17   \u001b[0m | \u001b[0m 0.8184  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 81.59   \u001b[0m | \u001b[0m 0.894   \u001b[0m | \u001b[0m 4.335   \u001b[0m | \u001b[0m 10.02   \u001b[0m | \u001b[0m 0.4045  \u001b[0m | \u001b[0m 4.838   \u001b[0m | \u001b[0m 96.61   \u001b[0m | \u001b[0m 0.7399  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 86.77   \u001b[0m | \u001b[0m 0.9043  \u001b[0m | \u001b[0m 4.166   \u001b[0m | \u001b[0m 10.84   \u001b[0m | \u001b[0m 0.09053 \u001b[0m | \u001b[0m 5.554   \u001b[0m | \u001b[0m 53.87   \u001b[0m | \u001b[0m 0.5649  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 84.72   \u001b[0m | \u001b[0m 0.9042  \u001b[0m | \u001b[0m 3.367   \u001b[0m | \u001b[0m 9.471   \u001b[0m | \u001b[0m 0.4316  \u001b[0m | \u001b[0m 5.821   \u001b[0m | \u001b[0m 57.27   \u001b[0m | \u001b[0m 0.6171  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 79.04   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.779   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 92.61   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 85.22   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.383   \u001b[0m | \u001b[0m 10.62   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 8.09    \u001b[0m | \u001b[0m 54.28   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 62.4    \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 5.052   \u001b[0m | \u001b[0m 12.48   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 98.8    \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 49.19   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.911   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 4.493   \u001b[0m | \u001b[0m 98.17   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 32.69   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 2.467   \u001b[0m | \u001b[0m 9.841   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 6.271   \u001b[0m | \u001b[0m 55.02   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 85.32   \u001b[0m | \u001b[0m 0.8925  \u001b[0m | \u001b[0m 3.497   \u001b[0m | \u001b[0m 9.248   \u001b[0m | \u001b[0m 0.2094  \u001b[0m | \u001b[0m 5.804   \u001b[0m | \u001b[0m 57.16   \u001b[0m | \u001b[0m 0.5723  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 86.79   \u001b[0m | \u001b[0m 0.9375  \u001b[0m | \u001b[0m 1.388   \u001b[0m | \u001b[0m 8.087   \u001b[0m | \u001b[0m 0.4095  \u001b[0m | \u001b[0m 6.948   \u001b[0m | \u001b[0m 95.74   \u001b[0m | \u001b[0m 0.7587  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 86.62   \u001b[0m | \u001b[0m 0.9554  \u001b[0m | \u001b[0m 1.446   \u001b[0m | \u001b[0m 9.405   \u001b[0m | \u001b[0m 0.3982  \u001b[0m | \u001b[0m 7.42    \u001b[0m | \u001b[0m 95.92   \u001b[0m | \u001b[0m 0.8024  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 75.82   \u001b[0m | \u001b[0m 0.9509  \u001b[0m | \u001b[0m 2.121   \u001b[0m | \u001b[0m 8.61    \u001b[0m | \u001b[0m 0.4128  \u001b[0m | \u001b[0m 8.087   \u001b[0m | \u001b[0m 96.23   \u001b[0m | \u001b[0m 0.6415  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 84.67   \u001b[0m | \u001b[0m 0.9229  \u001b[0m | \u001b[0m 1.372   \u001b[0m | \u001b[0m 8.923   \u001b[0m | \u001b[0m 0.3345  \u001b[0m | \u001b[0m 6.31    \u001b[0m | \u001b[0m 94.9    \u001b[0m | \u001b[0m 0.8231  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 84.49   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.766   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 7.22    \u001b[0m | \u001b[0m 94.4    \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 84.66   \u001b[0m | \u001b[0m 0.9292  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.38   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 6.947   \u001b[0m | \u001b[0m 96.87   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 82.82   \u001b[0m | \u001b[0m 0.9543  \u001b[0m | \u001b[0m 5.261   \u001b[0m | \u001b[0m 11.31   \u001b[0m | \u001b[0m 0.3803  \u001b[0m | \u001b[0m 5.381   \u001b[0m | \u001b[0m 53.18   \u001b[0m | \u001b[0m 0.4938  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 78.54   \u001b[0m | \u001b[0m 0.8785  \u001b[0m | \u001b[0m 1.557   \u001b[0m | \u001b[0m 10.64   \u001b[0m | \u001b[0m 0.1025  \u001b[0m | \u001b[0m 3.486   \u001b[0m | \u001b[0m 54.63   \u001b[0m | \u001b[0m 0.7181  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 86.74   \u001b[0m | \u001b[0m 0.9846  \u001b[0m | \u001b[0m 2.326   \u001b[0m | \u001b[0m 10.35   \u001b[0m | \u001b[0m 0.4646  \u001b[0m | \u001b[0m 8.338   \u001b[0m | \u001b[0m 29.66   \u001b[0m | \u001b[0m 0.6733  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 77.6    \u001b[0m | \u001b[0m 0.9376  \u001b[0m | \u001b[0m 2.655   \u001b[0m | \u001b[0m 8.765   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 7.923   \u001b[0m | \u001b[0m 29.8    \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 86.82   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 2.353   \u001b[0m | \u001b[0m 11.93   \u001b[0m | \u001b[0m 0.4208  \u001b[0m | \u001b[0m 8.625   \u001b[0m | \u001b[0m 29.4    \u001b[0m | \u001b[0m 0.4194  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 86.79   \u001b[0m | \u001b[0m 0.9684  \u001b[0m | \u001b[0m 1.088   \u001b[0m | \u001b[0m 10.74   \u001b[0m | \u001b[0m 0.4482  \u001b[0m | \u001b[0m 8.728   \u001b[0m | \u001b[0m 30.35   \u001b[0m | \u001b[0m 0.5267  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 86.87   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.18    \u001b[0m | \u001b[0m 10.65   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 28.72   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 24.79   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 2.562   \u001b[0m | \u001b[0m 11.3    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 9.0     \u001b[0m | \u001b[0m 30.92   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 80.55   \u001b[0m | \u001b[0m 0.9543  \u001b[0m | \u001b[0m 1.081   \u001b[0m | \u001b[0m 10.19   \u001b[0m | \u001b[0m 0.4297  \u001b[0m | \u001b[0m 7.539   \u001b[0m | \u001b[0m 29.45   \u001b[0m | \u001b[0m 0.6079  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 85.55   \u001b[0m | \u001b[0m 0.8514  \u001b[0m | \u001b[0m 2.288   \u001b[0m | \u001b[0m 12.88   \u001b[0m | \u001b[0m 0.3659  \u001b[0m | \u001b[0m 7.585   \u001b[0m | \u001b[0m 28.64   \u001b[0m | \u001b[0m 0.859   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 49.46   \u001b[0m | \u001b[0m 0.9011  \u001b[0m | \u001b[0m 2.026   \u001b[0m | \u001b[0m 11.29   \u001b[0m | \u001b[0m 0.02716 \u001b[0m | \u001b[0m 8.247   \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 0.7923  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 86.71   \u001b[0m | \u001b[0m 0.8994  \u001b[0m | \u001b[0m 1.906   \u001b[0m | \u001b[0m 7.589   \u001b[0m | \u001b[0m 0.1372  \u001b[0m | \u001b[0m 7.431   \u001b[0m | \u001b[0m 95.16   \u001b[0m | \u001b[0m 0.4108  \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 67.37   \u001b[0m | \u001b[0m 0.9228  \u001b[0m | \u001b[0m 1.098   \u001b[0m | \u001b[0m 10.37   \u001b[0m | \u001b[0m 0.244   \u001b[0m | \u001b[0m 6.839   \u001b[0m | \u001b[0m 95.37   \u001b[0m | \u001b[0m 0.4195  \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 78.55   \u001b[0m | \u001b[0m 0.8883  \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 8.661   \u001b[0m | \u001b[0m 0.4242  \u001b[0m | \u001b[0m 6.315   \u001b[0m | \u001b[0m 95.59   \u001b[0m | \u001b[0m 0.8817  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 48.88   \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 1.892   \u001b[0m | \u001b[0m 8.079   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 6.682   \u001b[0m | \u001b[0m 94.35   \u001b[0m | \u001b[0m 0.4     \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 70.89   \u001b[0m | \u001b[0m 0.9571  \u001b[0m | \u001b[0m 11.69   \u001b[0m | \u001b[0m 1.314   \u001b[0m | \u001b[0m 0.3924  \u001b[0m | \u001b[0m 8.784   \u001b[0m | \u001b[0m 69.29   \u001b[0m | \u001b[0m 0.6206  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 86.87   \u001b[0m | \u001b[0m 0.8762  \u001b[0m | \u001b[0m 1.037   \u001b[0m | \u001b[0m 7.418   \u001b[0m | \u001b[0m 0.05526 \u001b[0m | \u001b[0m 7.535   \u001b[0m | \u001b[0m 95.53   \u001b[0m | \u001b[0m 0.6029  \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 79.43   \u001b[0m | \u001b[0m 0.8621  \u001b[0m | \u001b[0m 1.413   \u001b[0m | \u001b[0m 7.217   \u001b[0m | \u001b[0m 0.49    \u001b[0m | \u001b[0m 6.917   \u001b[0m | \u001b[0m 95.34   \u001b[0m | \u001b[0m 0.8831  \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 86.71   \u001b[0m | \u001b[0m 0.8502  \u001b[0m | \u001b[0m 2.338   \u001b[0m | \u001b[0m 10.49   \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 6.238   \u001b[0m | \u001b[0m 99.74   \u001b[0m | \u001b[0m 0.6348  \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 63.76   \u001b[0m | \u001b[0m 0.9188  \u001b[0m | \u001b[0m 9.962   \u001b[0m | \u001b[0m 4.597   \u001b[0m | \u001b[0m 0.3619  \u001b[0m | \u001b[0m 4.476   \u001b[0m | \u001b[0m 78.3    \u001b[0m | \u001b[0m 0.5314  \u001b[0m |\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 21, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.9090386819155898, 12.634455264832026, 9.13566053676686, 0.4411598152684739, 8.721278319566448, 80.93130855012268, 0.5333975735601049)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-238400a233ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mproduct_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mproduct_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#####################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#####    |  target   |   alpha   |   col_k   |  col_k1   |   m_lr    | m_max_... | m_n_es... | m_subs... |\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-238400a233ba>\u001b[0m in \u001b[0;36mh_opt\u001b[0;34m(alpha, m_n_estimator, m_lr, m_subsample, m_max_depth, col_k, col_k1)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#model_ = KR(m_lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#acc_, acc_hat_, _, _, _, _ = run_model_(model_, d_set, 'Product', round(col_k), 201903, 202043, 202044, 202053)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_hat_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_set1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Product'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201945\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202102\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202102\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202112\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_k1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prev/ var :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc_hat_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-34b9fb672592>\u001b[0m in \u001b[0;36mrun_model_2\u001b[0;34m(m_, x_dta, target_key, wt_mat, tr_s_day, tr_e_day, te_s_day, te_e_day, col_k, col_k1)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mte_x_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m#print(te_x_.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtmp_y_hat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_x_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_y_hat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mtmp_y_hat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_y_hat_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m    824\u001b[0m                     \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                     \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                     \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 )\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_cupy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1839\u001b[0;31m                     \u001b[0;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m                     \u001b[0;34mf\"got {data.shape[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 21, got 1"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "param_bound = {'alpha' : (0.85,0.99) , 'm_n_estimator' : (10, 100), 'm_lr' : (0.01, 0.49), 'm_subsample' : (0.4, 0.9), 'm_max_depth' : (3,9), 'col_k' : (1,13), 'col_k1' : (1,13)}\n",
    "def h_opt(alpha, m_n_estimator, m_lr, m_subsample, m_max_depth, col_k, col_k1):\n",
    "    weight_mat = list(map(lambda x : round(alpha,3)**x if x > 0 else 1, range(0,12)))    #\n",
    "    model_=xgboost.XGBRegressor(n_estimators=round(m_n_estimator), learning_rate=round(m_lr,3), subsample=round(m_subsample,3), max_depth=round(m_max_depth), tree_method='gpu_hist', gpu_id=0)\n",
    "    #model_ = KR(m_lr)\n",
    "    #acc_, acc_hat_, _, _, _, _ = run_model_(model_, d_set, 'Product', round(col_k), 201903, 202043, 202044, 202053)\n",
    "    acc_, acc_hat_, _, _, _, _ = run_model_2(model_, d_set1, 'Product', weight_mat, 201945, 202102, 202102, 202112,  round(col_k), round(col_k1))  # prev/ var : \n",
    "    return acc_hat_    \n",
    "\n",
    "product_optimizer = BayesianOptimization(f=h_opt, pbounds=param_bound, verbose=2, random_state=1)\n",
    "product_optimizer.maximize(init_points=10, n_iter=100)\n",
    "#####################################################################################################################\n",
    "#####    |  target   |   alpha   |   col_k   |  col_k1   |   m_lr    | m_max_... | m_n_es... | m_subs... |\n",
    "#####    |  89.51    |  0.9372   |  2.36     |  1.64     |  0.486    |  6.004    |  18.09    |  0.5309   |\n",
    "#####    |  73.91    |  0.9197   |  10.75    |  3.53     |  0.4419   |  4.082    |  57.18    |  0.5262   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47217bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

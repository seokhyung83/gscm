{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "authorized-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os, random, time\n",
    "import xgboost\n",
    "import datetime\n",
    "import pygam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from fbprophet import Prophet\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thirty-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inven = pd.read_csv('./data/Y_Inven.csv')\n",
    "x_sales = pd.read_csv('./data/X_Sales.csv')\n",
    "x_product = pd.read_csv('./data/X_Product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knowing-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(x_dta, case=1, is_train=True):\n",
    "    x_prev_col = [s for s in x_dta.columns.values if 'Prev' in s]\n",
    "    x_post_col = [s for s in x_dta.columns.values if 'Post' in s]\n",
    "    x_var_col = [s for s in x_dta.columns.values if 'Var' in s]\n",
    "\n",
    "    X_prev_ = x_dta[x_prev_col]\n",
    "    X_post_ = x_dta[x_post_col]\n",
    "    X_var_ = x_dta[x_var_col]\n",
    "    if case ==1 :\n",
    "        Y_ =  x_dta['Sales']\n",
    "    else :\n",
    "        Y_ =  x_dta['Products']\n",
    "\n",
    "    if is_train :\n",
    "        X_prev_train = X_prev_[x_dta['YEAR']==2020]\n",
    "        X_post_train = X_post_[x_dta['YEAR']==2020]\n",
    "        X_var_train = X_var_[x_dta['YEAR']==2020]\n",
    "        Y_train = Y_[x_dta['YEAR']==2020]\n",
    "    else:\n",
    "        X_prev_train = X_prev_[x_dta['YEAR']==2021]\n",
    "        X_post_train = X_post_[x_dta['YEAR']==2021]\n",
    "        X_var_train = X_var_[x_dta['YEAR']==2021]\n",
    "        Y_train = Y_[x_dta['YEAR']==2021]\n",
    "    \n",
    "    return Y_train, X_prev_train, X_post_train, X_var_train    \n",
    "\n",
    "def run_model_(model_, trX_, trY_, teX_, teY_):\n",
    "    model_.fit(trX_, trY_)\n",
    "    hat_prev_ = model_.predict(trX_)\n",
    "    hat_  = model_.predict(teX_)\n",
    "    Y_hat_ = np.concatenate((hat_prev_, hat_))\n",
    "    Y_     = np.concatenate((trY_, teY_))\n",
    "    real_ = np.mean(1- np.abs(trY_ - hat_prev_) / np.abs(trY_)) * 100\n",
    "    fcst_ = np.mean(1- np.abs(teY_ - hat_     ) / np.abs(teY_)) * 100    \n",
    "    return real_, fcst_, Y_hat_, Y_, hat_prev_, hat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sales_train, sales_prev_X_train, sales_post_X_train, sales_var_X_train = dataset(x_sales, 1)\n",
    "Y_sales_test , sales_prev_X_test , sales_post_X_test , sales_var_X_test  = dataset(x_sales, 1, False)\n",
    "Y_products_train, products_prev_X_train, products_post_X_train, products_var_X_train = dataset(x_product, 2)\n",
    "Y_products_test , products_prev_X_test , products_post_X_test , products_var_X_test  = dataset(x_product, 2, False)\n",
    "sales_var_col = [s for s in sales_var_X_train.columns.values if 'Var' in s]\n",
    "product_var_col = [s for s in products_var_X_train.columns.values if 'Var' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "respected-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bound = {'alpha' : (0.9,0.99) , 'm_n_esitmator' : (10, 100), 'm_lr' : (0.01, 0.5), 'm_subsample' : (0.3, 0.9), 'm_max_depth' : (2,10), 'col_k' : (1,8)}\n",
    "def sales_opt(alpha, m_n_esitmator, m_lr, m_subsample, m_max_depth, col_k):\n",
    "    weight_mat = list(map(lambda x : alpha**x if x > 0 else 1, range(0,8)))\n",
    "    sales_var_X_train1 = np.multiply(sales_var_X_train, np.tile([weight_mat], sales_var_X_train.shape[0]).reshape(sales_var_X_train.shape[0], -1)).copy()\n",
    "    sales_var_X_test1 = np.multiply(sales_var_X_test, np.tile([weight_mat], sales_var_X_test.shape[0]).reshape(sales_var_X_test.shape[0], -1)).copy()\n",
    "    model_sales=xgboost.XGBRegressor(n_estimators=round(m_n_esitmator), learning_rate=m_lr, gamma=0, subsample=m_subsample, colsample_bytree=1, max_depth=round(m_max_depth))#, tree_method='gpu_hist', gpu_id=0)\n",
    "    \n",
    "    real_sale, fcst_sale, sales_Y_, sales_Y_hat_, sales_prev, sales_hat = run_model_(model_sales, \n",
    "                                                                                     sales_var_X_train1[sales_var_col[:round(col_k)]], Y_sales_train,\n",
    "                                                                                     sales_var_X_test1[sales_var_col[:round(col_k)]], Y_sales_test)\n",
    "    return fcst_sale\n",
    "    #print(\" Sales Mean Average => Train :  %f5 / Test : %f5\"%(real_sale, fcst_sale))   \n",
    "sales_optimizer = BayesianOptimization(f=sales_opt, pbounds=param_bound, verbose=2, random_state=1)\n",
    "sales_optimizer.maximize(init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-bowling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bound = {'alpha' : (0.9,0.99) , 'm_n_esitmator' : (10, 100), 'm_lr' : (0.01, 0.5), 'm_subsample' : (0.3, 0.9), 'm_max_depth' : (2,10), 'col_k' : (1,8)}\n",
    "def product_opt(alpha, m_n_esitmator, m_lr, m_subsample, m_max_depth, col_k):\n",
    "    weight_mat = list(map(lambda x : alpha**x if x > 0 else 1, range(0,8)))\n",
    "    \n",
    "    product_var_X_train1 = np.multiply(product_var_X_train, np.tile([weight_mat], product_var_X_train.shape[0]).reshape(product_var_X_train.shape[0], -1)).copy()\n",
    "    product_var_X_test1 = np.multiply(product_var_X_test, np.tile([weight_mat], product_var_X_test.shape[0]).reshape(product_var_X_test.shape[0], -1)).copy()\n",
    "    model_product=xgboost.XGBRegressor(n_estimators=round(m_n_esitmator), learning_rate=m_lr, gamma=0, subsample=m_subsample, colsample_bytree=1, max_depth=round(m_max_depth))#, tree_method='gpu_hist', gpu_id=0)\n",
    "    \n",
    "    real_product, fcst_product, product_Y_, product_Y_hat_, product_prev, product_hat = run_model_(model_product, \n",
    "                                                                                     product_var_X_train1[product_var_col[:round(col_k)]], Y_product_train,\n",
    "                                                                                     product_var_X_test1[product_var_col[:round(col_k)]], Y_product_test)\n",
    "    return fcst_product\n",
    "    #print(\" Sales Mean Average => Train :  %f5 / Test : %f5\"%(real_sale, fcst_sale))   \n",
    "product_optimizer = BayesianOptimization(f=product_opt, pbounds=param_bound, verbose=2, random_state=1)\n",
    "product_optimizer.maximize(init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-mitchell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infinite-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sales Mean Average => Train :  75.0710395 / Test : 90.3850035\n",
      " Product Mean Average => Train :  -114.1519345 / Test : 89.5066285\n"
     ]
    }
   ],
   "source": [
    "##        iter      |  target   |   alpha   |   col_k   |   m_lr    | m_max_... | m_n_es... | m_subs... |\n",
    "## Sales   55       |  90.38    |  0.9526   |  4.938    |  0.06102  |  9.038    |  68.36    |  0.3951   |\n",
    "## Product 24       |  89.73    |  0.9778   |  2.284    |  0.1653   |  2.388    |  90.34    |  0.4241   |\n",
    "sales_weight = list(map(lambda x : 0.9526**x if x > 0 else 1, range(0,8)))\n",
    "product_weight = list(map(lambda x : 0.9778**x if x > 0 else 1, range(0,8)))\n",
    "sales_var_X_train1 = np.multiply(sales_var_X_train, np.tile([sales_weight], sales_var_X_train.shape[0]).reshape(sales_var_X_train.shape[0], -1)).copy()\n",
    "sales_var_X_test1 = np.multiply(sales_var_X_test, np.tile([sales_weight], sales_var_X_test.shape[0]).reshape(sales_var_X_test.shape[0], -1)).copy()\n",
    "products_var_X_train1 = np.multiply(products_var_X_train, np.tile([product_weight], products_var_X_train.shape[0]).reshape(products_var_X_train.shape[0], -1)).copy()\n",
    "products_var_X_test1 = np.multiply(products_var_X_test, np.tile([product_weight], products_var_X_test.shape[0]).reshape(products_var_X_test.shape[0], -1)).copy()\n",
    "\n",
    "model_sales=xgboost.XGBRegressor(n_estimators=68, learning_rate=0.06102, gamma=0, subsample=0.3951, colsample_bytree=1, max_depth=9)#, tree_method='gpu_hist', gpu_id=0)\n",
    "model_products=xgboost.XGBRegressor(n_estimators=50, learning_rate=0.1653, gamma=0, subsample=0.4241, colsample_bytree=1, max_depth=2)#, tree_method='gpu_hist', gpu_id=0)\n",
    "sales_var_col = [s for s in sales_var_X_train.columns.values if 'Var' in s]\n",
    "product_var_col = [s for s in products_var_X_train.columns.values if 'Var' in s]\n",
    "\n",
    "real_sale, fcst_sale, sales_Y_, sales_Y_hat_, sales_prev, sales_hat = run_model_(model_sales, \n",
    "                                                                                 sales_var_X_train1[sales_var_col[:5]], Y_sales_train,\n",
    "                                                                                 sales_var_X_test1[sales_var_col[:5]], Y_sales_test)\n",
    "real_product, fcst_product, products_Y_, products_Y_hat_, products_prev, products_hat = run_model_(model_products, \n",
    "                                                                                                   products_var_X_train1[product_var_col[:2]], Y_products_train,\n",
    "                                                                                                   products_var_X_test1[product_var_col[:2]], Y_products_test)\n",
    "\n",
    "print(\" Sales Mean Average => Train :  %f5 / Test : %f5\"%(real_sale, fcst_sale))\n",
    "print(\" Product Mean Average => Train :  %f5 / Test : %f5\"%(real_product, fcst_product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-skirt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "musical-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inven_train = y_inven[y_inven['YEAR']==2020]\n",
    "y_inven_test = y_inven[y_inven['YEAR']==2021]\n",
    "\n",
    "products_prev_x_test_hat = pd.concat([products_prev_X_test.iloc[:12].reset_index(drop=True), pd.DataFrame(products_hat[:12])], 1).copy()\n",
    "for i in range(products_prev_x_test_hat.shape[0]):\n",
    "    tmp_prod_hat = products_prev_x_test_hat.iloc[i, -1]\n",
    "    for j in range(0, 8):\n",
    "        if i+j+1 < products_prev_x_test_hat.shape[0]:\n",
    "            products_prev_x_test_hat.iloc[i+j+1, j ] = tmp_prod_hat\n",
    "            \n",
    "            \n",
    "dta_train = pd.concat([y_inven_train.iloc[1:, :], y_inven_test.iloc[:1, :]]).copy().reset_index(drop=True)\n",
    "dta_train['Sales'] = Y_sales_train\n",
    "dta_train = pd.concat([dta_train, products_prev_X_train], axis=1).copy()\n",
    "dta_train['Product'] = Y_products_train\n",
    "#dta_train = pd.concat([dta_train, products_prev_X_train], axis=1)\n",
    "\n",
    "dta_test = y_inven_test.iloc[1:,:].copy()\n",
    "dta_test['Sales'] = sales_hat[:12]\n",
    "dta_test = pd.concat([dta_test.reset_index(drop=True), products_prev_x_test_hat.iloc[:,:-1]], axis=1).copy()\n",
    "dta_test['Product'] = products_hat[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "iraqi-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |  col_k1   |  col_k2   |   m_lr    | m_max_... | m_n_es... | m_subs... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 16.47   \u001b[0m | \u001b[0m 3.336   \u001b[0m | \u001b[0m 5.763   \u001b[0m | \u001b[0m 0.01006 \u001b[0m | \u001b[0m 4.419   \u001b[0m | \u001b[0m 23.21   \u001b[0m | \u001b[0m 0.3554  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 86.7    \u001b[0m | \u001b[95m 1.49    \u001b[0m | \u001b[95m 2.764   \u001b[0m | \u001b[95m 0.2044  \u001b[0m | \u001b[95m 6.311   \u001b[0m | \u001b[95m 47.73   \u001b[0m | \u001b[95m 0.7111  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 51.35   \u001b[0m | \u001b[0m 1.636   \u001b[0m | \u001b[0m 7.025   \u001b[0m | \u001b[0m 0.02342 \u001b[0m | \u001b[0m 7.364   \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 0.6352  \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 91.68   \u001b[0m | \u001b[95m 1.123   \u001b[0m | \u001b[95m 1.585   \u001b[0m | \u001b[95m 0.4024  \u001b[0m | \u001b[95m 9.746   \u001b[0m | \u001b[95m 38.21   \u001b[0m | \u001b[95m 0.7154  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 57.24   \u001b[0m | \u001b[0m 7.011   \u001b[0m | \u001b[0m 7.157   \u001b[0m | \u001b[0m 0.05167 \u001b[0m | \u001b[0m 2.312   \u001b[0m | \u001b[0m 25.28   \u001b[0m | \u001b[0m 0.8269  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 85.12   \u001b[0m | \u001b[0m 0.7868  \u001b[0m | \u001b[0m 3.369   \u001b[0m | \u001b[0m 0.4794  \u001b[0m | \u001b[0m 6.265   \u001b[0m | \u001b[0m 72.27   \u001b[0m | \u001b[0m 0.4893  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 83.88   \u001b[0m | \u001b[0m 5.492   \u001b[0m | \u001b[0m 6.677   \u001b[0m | \u001b[0m 0.01896 \u001b[0m | \u001b[0m 8.001   \u001b[0m | \u001b[0m 99.0    \u001b[0m | \u001b[0m 0.7489  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 81.43   \u001b[0m | \u001b[0m 2.244   \u001b[0m | \u001b[0m 6.314   \u001b[0m | \u001b[0m 0.06058 \u001b[0m | \u001b[0m 5.583   \u001b[0m | \u001b[0m 91.77   \u001b[0m | \u001b[0m 0.4762  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 34.51   \u001b[0m | \u001b[0m 2.302   \u001b[0m | \u001b[0m 1.04    \u001b[0m | \u001b[0m 0.01949 \u001b[0m | \u001b[0m 7.431   \u001b[0m | \u001b[0m 29.05   \u001b[0m | \u001b[0m 0.4593  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 87.28   \u001b[0m | \u001b[0m 3.933   \u001b[0m | \u001b[0m 0.4269  \u001b[0m | \u001b[0m 0.2913  \u001b[0m | \u001b[0m 3.174   \u001b[0m | \u001b[0m 63.04   \u001b[0m | \u001b[0m 0.7199  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 79.66   \u001b[0m | \u001b[0m 5.467   \u001b[0m | \u001b[0m 6.26    \u001b[0m | \u001b[0m 0.06522 \u001b[0m | \u001b[0m 8.125   \u001b[0m | \u001b[0m 99.17   \u001b[0m | \u001b[0m 0.5829  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 64.96   \u001b[0m | \u001b[0m 1.11    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.4806  \u001b[0m | \u001b[0m 7.524   \u001b[0m | \u001b[0m 42.93   \u001b[0m | \u001b[0m 0.7872  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 46.95   \u001b[0m | \u001b[0m 5.818   \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.614   \u001b[0m | \u001b[0m 95.46   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 90.15   \u001b[0m | \u001b[0m 1.041   \u001b[0m | \u001b[0m 4.12    \u001b[0m | \u001b[0m 0.4652  \u001b[0m | \u001b[0m 9.421   \u001b[0m | \u001b[0m 37.28   \u001b[0m | \u001b[0m 0.7577  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 82.96   \u001b[0m | \u001b[0m 3.812   \u001b[0m | \u001b[0m 2.73    \u001b[0m | \u001b[0m 0.1516  \u001b[0m | \u001b[0m 8.077   \u001b[0m | \u001b[0m 38.2    \u001b[0m | \u001b[0m 0.354   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 27.75   \u001b[0m | \u001b[0m 1.083   \u001b[0m | \u001b[0m 4.28    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 40.49   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 80.25   \u001b[0m | \u001b[0m 1.853   \u001b[0m | \u001b[0m 2.286   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.936   \u001b[0m | \u001b[0m 36.49   \u001b[0m | \u001b[0m 0.8557  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 87.95   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 2.497   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 36.77   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 80.15   \u001b[0m | \u001b[0m 0.2214  \u001b[0m | \u001b[0m 5.262   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 35.15   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 80.55   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.924   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.005   \u001b[0m | \u001b[0m 36.83   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 87.12   \u001b[0m | \u001b[0m 3.474   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 38.34   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 81.13   \u001b[0m | \u001b[0m 1.531   \u001b[0m | \u001b[0m 0.41    \u001b[0m | \u001b[0m 0.2717  \u001b[0m | \u001b[0m 5.052   \u001b[0m | \u001b[0m 49.47   \u001b[0m | \u001b[0m 0.7693  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 90.0    \u001b[0m | \u001b[0m 2.091   \u001b[0m | \u001b[0m 2.043   \u001b[0m | \u001b[0m 0.2958  \u001b[0m | \u001b[0m 4.112   \u001b[0m | \u001b[0m 65.38   \u001b[0m | \u001b[0m 0.7573  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 82.74   \u001b[0m | \u001b[0m 4.477   \u001b[0m | \u001b[0m 0.09422 \u001b[0m | \u001b[0m 0.4326  \u001b[0m | \u001b[0m 4.092   \u001b[0m | \u001b[0m 66.54   \u001b[0m | \u001b[0m 0.4732  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 84.59   \u001b[0m | \u001b[0m 1.492   \u001b[0m | \u001b[0m 0.511   \u001b[0m | \u001b[0m 0.4202  \u001b[0m | \u001b[0m 6.455   \u001b[0m | \u001b[0m 63.32   \u001b[0m | \u001b[0m 0.5845  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 86.74   \u001b[0m | \u001b[0m 1.129   \u001b[0m | \u001b[0m 1.901   \u001b[0m | \u001b[0m 0.4666  \u001b[0m | \u001b[0m 6.531   \u001b[0m | \u001b[0m 68.09   \u001b[0m | \u001b[0m 0.5116  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 86.55   \u001b[0m | \u001b[0m 4.436   \u001b[0m | \u001b[0m 3.264   \u001b[0m | \u001b[0m 0.1984  \u001b[0m | \u001b[0m 6.279   \u001b[0m | \u001b[0m 64.46   \u001b[0m | \u001b[0m 0.3568  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 72.23   \u001b[0m | \u001b[0m 3.256   \u001b[0m | \u001b[0m 4.856   \u001b[0m | \u001b[0m 0.4247  \u001b[0m | \u001b[0m 5.598   \u001b[0m | \u001b[0m 67.8    \u001b[0m | \u001b[0m 0.3892  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 38.33   \u001b[0m | \u001b[0m 2.22    \u001b[0m | \u001b[0m 3.674   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 4.029   \u001b[0m | \u001b[0m 61.94   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 35.44   \u001b[0m | \u001b[0m 3.244   \u001b[0m | \u001b[0m 0.8145  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 6.983   \u001b[0m | \u001b[0m 65.8    \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 87.17   \u001b[0m | \u001b[0m 3.112   \u001b[0m | \u001b[0m 0.8116  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.8     \u001b[0m | \u001b[0m 65.1    \u001b[0m | \u001b[0m 0.5973  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 83.48   \u001b[0m | \u001b[0m 1.09    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 37.15   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 80.9    \u001b[0m | \u001b[0m 1.302   \u001b[0m | \u001b[0m 1.702   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.737   \u001b[0m | \u001b[0m 67.5    \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 89.08   \u001b[0m | \u001b[0m 4.809   \u001b[0m | \u001b[0m 2.492   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.678   \u001b[0m | \u001b[0m 65.16   \u001b[0m | \u001b[0m 0.3959  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 90.99   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.663   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.946   \u001b[0m | \u001b[0m 70.22   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 75.31   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.427   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.667   \u001b[0m | \u001b[0m 69.68   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 83.3    \u001b[0m | \u001b[0m 2.215   \u001b[0m | \u001b[0m 1.472   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.341   \u001b[0m | \u001b[0m 70.7    \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 88.64   \u001b[0m | \u001b[0m 6.176   \u001b[0m | \u001b[0m 0.3475  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.426   \u001b[0m | \u001b[0m 64.56   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 70.01   \u001b[0m | \u001b[0m 6.87    \u001b[0m | \u001b[0m 1.959   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.762   \u001b[0m | \u001b[0m 63.59   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 79.86   \u001b[0m | \u001b[0m 0.2106  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.997   \u001b[0m | \u001b[0m 64.36   \u001b[0m | \u001b[0m 0.7403  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 52.01   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.15    \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 4.003   \u001b[0m | \u001b[0m 70.53   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 64.27   \u001b[0m | \u001b[0m 0.8739  \u001b[0m | \u001b[0m 0.538   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.42    \u001b[0m | \u001b[0m 70.86   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 34.4    \u001b[0m | \u001b[0m 5.08    \u001b[0m | \u001b[0m 1.446   \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 66.08   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 83.38   \u001b[0m | \u001b[0m 3.676   \u001b[0m | \u001b[0m 1.509   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.144   \u001b[0m | \u001b[0m 64.44   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 86.87   \u001b[0m | \u001b[0m 0.3424  \u001b[0m | \u001b[0m 2.579   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.6     \u001b[0m | \u001b[0m 37.79   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 80.74   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.8949  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.532   \u001b[0m | \u001b[0m 68.68   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 84.12   \u001b[0m | \u001b[0m 2.901   \u001b[0m | \u001b[0m 1.822   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 38.03   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 79.53   \u001b[0m | \u001b[0m 2.119   \u001b[0m | \u001b[0m 0.5931  \u001b[0m | \u001b[0m 0.3388  \u001b[0m | \u001b[0m 8.457   \u001b[0m | \u001b[0m 38.41   \u001b[0m | \u001b[0m 0.4611  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 82.23   \u001b[0m | \u001b[0m 5.287   \u001b[0m | \u001b[0m 4.2     \u001b[0m | \u001b[0m 0.1288  \u001b[0m | \u001b[0m 4.858   \u001b[0m | \u001b[0m 65.78   \u001b[0m | \u001b[0m 0.3816  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 80.47   \u001b[0m | \u001b[0m 3.352   \u001b[0m | \u001b[0m 3.406   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.265   \u001b[0m | \u001b[0m 65.61   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 77.61   \u001b[0m | \u001b[0m 1.201   \u001b[0m | \u001b[0m 1.485   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.566   \u001b[0m | \u001b[0m 65.23   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 82.33   \u001b[0m | \u001b[0m 2.185   \u001b[0m | \u001b[0m 4.975   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.296   \u001b[0m | \u001b[0m 36.37   \u001b[0m | \u001b[0m 0.6407  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 29.49   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.7     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 8.889   \u001b[0m | \u001b[0m 36.03   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 81.36   \u001b[0m | \u001b[0m 1.198   \u001b[0m | \u001b[0m 2.846   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 37.66   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 85.51   \u001b[0m | \u001b[0m 0.01097 \u001b[0m | \u001b[0m 1.316   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.567   \u001b[0m | \u001b[0m 37.52   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 77.24   \u001b[0m | \u001b[0m 1.22    \u001b[0m | \u001b[0m 2.117   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.962   \u001b[0m | \u001b[0m 69.55   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 83.59   \u001b[0m | \u001b[0m 1.933   \u001b[0m | \u001b[0m 3.73    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.423   \u001b[0m | \u001b[0m 37.79   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 25.96   \u001b[0m | \u001b[0m 2.134   \u001b[0m | \u001b[0m 0.8541  \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 37.48   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 75.15   \u001b[0m | \u001b[0m 2.831   \u001b[0m | \u001b[0m 1.622   \u001b[0m | \u001b[0m 0.4294  \u001b[0m | \u001b[0m 3.587   \u001b[0m | \u001b[0m 65.18   \u001b[0m | \u001b[0m 0.5883  \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 85.45   \u001b[0m | \u001b[0m 0.2714  \u001b[0m | \u001b[0m 2.44    \u001b[0m | \u001b[0m 0.0367  \u001b[0m | \u001b[0m 6.779   \u001b[0m | \u001b[0m 80.57   \u001b[0m | \u001b[0m 0.4596  \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 89.12   \u001b[0m | \u001b[0m 3.607   \u001b[0m | \u001b[0m 5.126   \u001b[0m | \u001b[0m 0.3636  \u001b[0m | \u001b[0m 3.242   \u001b[0m | \u001b[0m 26.99   \u001b[0m | \u001b[0m 0.7086  \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 83.54   \u001b[0m | \u001b[0m 1.164   \u001b[0m | \u001b[0m 2.742   \u001b[0m | \u001b[0m 0.2937  \u001b[0m | \u001b[0m 9.158   \u001b[0m | \u001b[0m 37.78   \u001b[0m | \u001b[0m 0.503   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 88.25   \u001b[0m | \u001b[0m 0.8339  \u001b[0m | \u001b[0m 1.318   \u001b[0m | \u001b[0m 0.1538  \u001b[0m | \u001b[0m 9.17    \u001b[0m | \u001b[0m 38.67   \u001b[0m | \u001b[0m 0.8938  \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 82.34   \u001b[0m | \u001b[0m 0.8924  \u001b[0m | \u001b[0m 1.662   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.896   \u001b[0m | \u001b[0m 37.77   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 83.35   \u001b[0m | \u001b[0m 1.507   \u001b[0m | \u001b[0m 2.132   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.409   \u001b[0m | \u001b[0m 38.87   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 84.86   \u001b[0m | \u001b[0m 3.294   \u001b[0m | \u001b[0m 0.7381  \u001b[0m | \u001b[0m 0.2281  \u001b[0m | \u001b[0m 3.111   \u001b[0m | \u001b[0m 64.05   \u001b[0m | \u001b[0m 0.7526  \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 81.8    \u001b[0m | \u001b[0m 4.62    \u001b[0m | \u001b[0m 2.861   \u001b[0m | \u001b[0m 0.3537  \u001b[0m | \u001b[0m 4.842   \u001b[0m | \u001b[0m 64.83   \u001b[0m | \u001b[0m 0.3545  \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 81.38   \u001b[0m | \u001b[0m 0.3619  \u001b[0m | \u001b[0m 2.25    \u001b[0m | \u001b[0m 0.4067  \u001b[0m | \u001b[0m 9.486   \u001b[0m | \u001b[0m 38.62   \u001b[0m | \u001b[0m 0.5822  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 79.78   \u001b[0m | \u001b[0m 4.804   \u001b[0m | \u001b[0m 0.6129  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.225   \u001b[0m | \u001b[0m 64.1    \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 88.98   \u001b[0m | \u001b[0m 1.329   \u001b[0m | \u001b[0m 2.4     \u001b[0m | \u001b[0m 0.3556  \u001b[0m | \u001b[0m 4.538   \u001b[0m | \u001b[0m 66.1    \u001b[0m | \u001b[0m 0.6674  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 80.45   \u001b[0m | \u001b[0m 1.149   \u001b[0m | \u001b[0m 2.698   \u001b[0m | \u001b[0m 0.487   \u001b[0m | \u001b[0m 4.027   \u001b[0m | \u001b[0m 64.88   \u001b[0m | \u001b[0m 0.4637  \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 83.89   \u001b[0m | \u001b[0m 2.218   \u001b[0m | \u001b[0m 4.354   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.7     \u001b[0m | \u001b[0m 37.15   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 83.97   \u001b[0m | \u001b[0m 1.313   \u001b[0m | \u001b[0m 5.201   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.946   \u001b[0m | \u001b[0m 37.67   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 85.84   \u001b[0m | \u001b[0m 3.912   \u001b[0m | \u001b[0m 1.278   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.484   \u001b[0m | \u001b[0m 38.92   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[95m 75      \u001b[0m | \u001b[95m 92.53   \u001b[0m | \u001b[95m 0.6479  \u001b[0m | \u001b[95m 1.956   \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 5.485   \u001b[0m | \u001b[95m 67.09   \u001b[0m | \u001b[95m 0.767   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 87.41   \u001b[0m | \u001b[0m 0.2423  \u001b[0m | \u001b[0m 1.92    \u001b[0m | \u001b[0m 0.4654  \u001b[0m | \u001b[0m 5.351   \u001b[0m | \u001b[0m 65.55   \u001b[0m | \u001b[0m 0.6377  \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 74.34   \u001b[0m | \u001b[0m 0.8285  \u001b[0m | \u001b[0m 2.857   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 6.06    \u001b[0m | \u001b[0m 66.57   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 81.49   \u001b[0m | \u001b[0m 0.9295  \u001b[0m | \u001b[0m 1.18    \u001b[0m | \u001b[0m 0.02996 \u001b[0m | \u001b[0m 3.9     \u001b[0m | \u001b[0m 65.88   \u001b[0m | \u001b[0m 0.584   \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 86.26   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 2.075   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.472   \u001b[0m | \u001b[0m 66.78   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 84.64   \u001b[0m | \u001b[0m 0.3052  \u001b[0m | \u001b[0m 0.3298  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 38.68   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 85.65   \u001b[0m | \u001b[0m 0.3189  \u001b[0m | \u001b[0m 0.7657  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.499   \u001b[0m | \u001b[0m 66.67   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 82.48   \u001b[0m | \u001b[0m 3.29    \u001b[0m | \u001b[0m 4.867   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.771   \u001b[0m | \u001b[0m 28.33   \u001b[0m | \u001b[0m 0.8327  \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 84.9    \u001b[0m | \u001b[0m 3.951   \u001b[0m | \u001b[0m 2.895   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.854   \u001b[0m | \u001b[0m 38.23   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 76.55   \u001b[0m | \u001b[0m 4.532   \u001b[0m | \u001b[0m 4.391   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.903   \u001b[0m | \u001b[0m 27.56   \u001b[0m | \u001b[0m 0.3721  \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 83.52   \u001b[0m | \u001b[0m 2.265   \u001b[0m | \u001b[0m 5.535   \u001b[0m | \u001b[0m 0.1267  \u001b[0m | \u001b[0m 3.744   \u001b[0m | \u001b[0m 27.14   \u001b[0m | \u001b[0m 0.8159  \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 84.53   \u001b[0m | \u001b[0m 6.347   \u001b[0m | \u001b[0m 4.395e-0\u001b[0m | \u001b[0m 0.103   \u001b[0m | \u001b[0m 2.833   \u001b[0m | \u001b[0m 63.37   \u001b[0m | \u001b[0m 0.8623  \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 83.73   \u001b[0m | \u001b[0m 2.814   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.428   \u001b[0m | \u001b[0m 63.16   \u001b[0m | \u001b[0m 0.4461  \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 87.87   \u001b[0m | \u001b[0m 4.945   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 38.63   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 83.38   \u001b[0m | \u001b[0m 2.994   \u001b[0m | \u001b[0m 5.755   \u001b[0m | \u001b[0m 0.3771  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 26.82   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 83.96   \u001b[0m | \u001b[0m 4.883   \u001b[0m | \u001b[0m 1.336   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 37.64   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 83.3    \u001b[0m | \u001b[0m 1.692   \u001b[0m | \u001b[0m 1.32    \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 5.228   \u001b[0m | \u001b[0m 67.56   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 85.13   \u001b[0m | \u001b[0m 3.721   \u001b[0m | \u001b[0m 6.569   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.391   \u001b[0m | \u001b[0m 27.58   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 88.84   \u001b[0m | \u001b[0m 3.128   \u001b[0m | \u001b[0m 0.1276  \u001b[0m | \u001b[0m 0.3342  \u001b[0m | \u001b[0m 9.131   \u001b[0m | \u001b[0m 39.81   \u001b[0m | \u001b[0m 0.8679  \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 89.57   \u001b[0m | \u001b[0m 4.656   \u001b[0m | \u001b[0m 0.4559  \u001b[0m | \u001b[0m 0.3185  \u001b[0m | \u001b[0m 9.797   \u001b[0m | \u001b[0m 40.18   \u001b[0m | \u001b[0m 0.8683  \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 74.31   \u001b[0m | \u001b[0m 3.704   \u001b[0m | \u001b[0m 1.413   \u001b[0m | \u001b[0m 0.03556 \u001b[0m | \u001b[0m 9.711   \u001b[0m | \u001b[0m 40.88   \u001b[0m | \u001b[0m 0.5121  \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 84.93   \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 1.446   \u001b[0m | \u001b[0m 0.2062  \u001b[0m | \u001b[0m 9.879   \u001b[0m | \u001b[0m 39.48   \u001b[0m | \u001b[0m 0.7364  \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 87.75   \u001b[0m | \u001b[0m 4.484   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 8.501   \u001b[0m | \u001b[0m 39.24   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 70.07   \u001b[0m | \u001b[0m 7.616   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 64.2    \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 84.55   \u001b[0m | \u001b[0m 6.02    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 9.102   \u001b[0m | \u001b[0m 40.01   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 79.86   \u001b[0m | \u001b[0m 2.59    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 64.19   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 83.1    \u001b[0m | \u001b[0m 5.574   \u001b[0m | \u001b[0m 1.403   \u001b[0m | \u001b[0m 0.1956  \u001b[0m | \u001b[0m 8.153   \u001b[0m | \u001b[0m 37.68   \u001b[0m | \u001b[0m 0.446   \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 83.54   \u001b[0m | \u001b[0m 5.176   \u001b[0m | \u001b[0m 1.489   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.994   \u001b[0m | \u001b[0m 39.74   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m 84.64   \u001b[0m | \u001b[0m 0.2989  \u001b[0m | \u001b[0m 1.599   \u001b[0m | \u001b[0m 0.2437  \u001b[0m | \u001b[0m 6.289   \u001b[0m | \u001b[0m 48.3    \u001b[0m | \u001b[0m 0.7171  \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m 80.24   \u001b[0m | \u001b[0m 2.14    \u001b[0m | \u001b[0m 1.405   \u001b[0m | \u001b[0m 0.369   \u001b[0m | \u001b[0m 6.887   \u001b[0m | \u001b[0m 48.42   \u001b[0m | \u001b[0m 0.5299  \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m 87.21   \u001b[0m | \u001b[0m 2.849   \u001b[0m | \u001b[0m 0.211   \u001b[0m | \u001b[0m 0.358   \u001b[0m | \u001b[0m 7.154   \u001b[0m | \u001b[0m 39.51   \u001b[0m | \u001b[0m 0.8719  \u001b[0m |\n",
      "| \u001b[0m 106     \u001b[0m | \u001b[0m 87.53   \u001b[0m | \u001b[0m 4.057   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.968   \u001b[0m | \u001b[0m 40.77   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m 85.64   \u001b[0m | \u001b[0m 3.908   \u001b[0m | \u001b[0m 0.9263  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.141   \u001b[0m | \u001b[0m 38.32   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m 83.91   \u001b[0m | \u001b[0m 3.202   \u001b[0m | \u001b[0m 1.555   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.779   \u001b[0m | \u001b[0m 39.76   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 4.495   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 6.773   \u001b[0m | \u001b[0m 39.72   \u001b[0m | \u001b[0m 0.3     \u001b[0m |\n",
      "| \u001b[0m 110     \u001b[0m | \u001b[0m 85.03   \u001b[0m | \u001b[0m 3.927   \u001b[0m | \u001b[0m 1.629   \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 7.677   \u001b[0m | \u001b[0m 36.92   \u001b[0m | \u001b[0m 0.9     \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "param_bound = {'m_n_estimator' : (10, 100), 'm_lr' : (0.01, 0.5), 'm_subsample' : (0.3, 0.9), \n",
    "               'm_max_depth' : (2,10), 'col_k1' : (0,8), 'col_k2' : (0,8)}\n",
    "def inven_opt(m_n_estimator, m_lr, m_subsample, m_max_depth, col_k1, col_k2):    \n",
    "    \n",
    "    i_col = round(col_k1)\n",
    "    p_col = round(col_k2)\n",
    "    p_m_n_estimator = round(m_n_estimator)\n",
    "    p_m_lr = round(m_lr, 2)\n",
    "    p_m_sub = round(m_subsample, 2)\n",
    "    p_m_max_dep = round(m_max_depth)\n",
    "    \n",
    "    prev_inven_col   = [s for s in dta_test.columns.values if 'prev_INVENTORY' in s]\n",
    "    prev_product_col = [s for s in dta_test.columns.values if 'Prev_Product' in s]\n",
    "    train_col_name = list(['Sales', 'Product'])+prev_inven_col[:i_col]+prev_product_col[:p_col]\n",
    "    \n",
    "    model_inven=xgboost.XGBRegressor(n_estimators=p_m_n_estimator, learning_rate=p_m_lr, gamma=0, subsample=p_m_sub, \n",
    "                                     colsample_bytree=1, max_depth=p_m_max_dep)\n",
    "\n",
    "    model_inven.fit(dta_train[train_col_name], dta_train['INVENTORY'])\n",
    "    inven_prev = model_inven.predict(dta_train[train_col_name])\n",
    "\n",
    "    inven_hat = [] \n",
    "    for i in range(0, dta_test.shape[0]):\n",
    "        test_default_x = dta_test[['Sales', 'Product']].iloc[i:(i+1),:]\n",
    "        test_prev_inven_x = dta_test[prev_inven_col[:i_col]].iloc[i:(i+1),:]\n",
    "        test_prev_product_x = dta_test[prev_product_col[:p_col]].iloc[i:(i+1),:]\n",
    "\n",
    "        if i > 0 :        \n",
    "            for j in range(0, i_col):\n",
    "                if j < len(inven_hat):\n",
    "                    test_prev_inven_x.iloc[:1,j] = inven_hat[(len(inven_hat)-j-1)]\n",
    "\n",
    "        test_x = pd.concat([test_default_x, test_prev_inven_x, test_prev_product_x], 1)                              \n",
    "        inven_hat.append(model_inven.predict(test_x)[0])\n",
    "\n",
    "    Y_inven_hat = np.concatenate((inven_prev, inven_hat))\n",
    "    Y_inven = np.concatenate((dta_train['INVENTORY'], dta_test['INVENTORY']))\n",
    "\n",
    "    real_avg = np.mean(1- np.abs(dta_train['INVENTORY'] - inven_prev) / dta_train['INVENTORY']) * 100\n",
    "    fcst_avg = np.mean(1- np.abs(dta_test['INVENTORY'] - inven_hat) / dta_test['INVENTORY']) * 100 \n",
    "    \n",
    "    return fcst_avg\n",
    "\n",
    "inven_optimizer = BayesianOptimization(f=inven_opt, pbounds=param_bound, verbose=2, random_state=1)\n",
    "inven_optimizer.maximize(init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-junior",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGí™”í•™ í•™ìŠµë°ì´í„° ìƒì„±  \n",
    "lg_chem í´ë” ë‚´ì— db.sqlite, ë¶ˆìš©ì–´ íŒŒì¼, íƒœê·¸ íŒŒì¼(tag.xlsxë¡œ íŒŒì¼ëª… ë³€í™˜), dx_tag í´ë”ë¥¼ ë„£ì€ ìƒíƒœì—ì„œ ëŒë¦¬ë©´ ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinho.lee\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "# from pororo import Pororo\n",
    "# prr = Pororo(task=\"tokenization\", lang=\"ko\", model=\"mecab_ko\")\n",
    "# prr = Pororo(task=\"tokenization\", lang=\"ko\", model=\"sent_ko\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. íƒœê·¸ ì‚¬ì „ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path        = './'\n",
    "dx_path        = './dx_tag/'\n",
    "data_path      = \"./chem_data/\"\n",
    "tag_data_path  = \"./tag_data/\"\n",
    "stopwords_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data_df = pd.read_excel('./tag.xlsx')\n",
    "tag_data_df = tag_data_df.drop_duplicates(['í‚¤ì›Œë“œ'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst = tag_data_df['TAG'][-tag_data_df['TAG'].isna()].unique().tolist()\n",
    "# ['ê¸°ê´€', 'ê¸°ìˆ ', 'ì†Œì¬', 'ì—…ì²´', 'ì´ë²¤íŠ¸', 'ì´ìŠˆ', 'ì •ì±…', 'ì œí’ˆ']\n",
    "\n",
    "global label_dict # í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©. LGí™”í•™ì˜ ê²½ìš° íƒœê·¸ê°€ í•œê¸€ë¡œ ë˜ì–´ìˆì–´ì„œ ë³€ê²½í•˜ê³ ì í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "# 'LABEL-C'ì€ í™”í•™ ê³ ìœ ì˜ íƒœê·¸ì´ê³ , DX tagì™€ mergeëœ '-C'ê°€ ë¹ ì§„ íŒŒì¼ëª…ì„ ê°€ì§ˆ ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "label_dict = {'ê¸°ê´€' : 'ORG-C', 'ê¸°ìˆ ' : 'TE-C', 'ì†Œì¬' : 'MAT', 'ì—…ì²´' : 'COM-C', 'ì´ë²¤íŠ¸' : 'EVT-C', 'ì´ìŠˆ' : 'ISS', 'ì •ì±…' : 'POL-C', 'ì œí’ˆ' : 'PRD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ref_dic(label_lst, dic_data_df):\n",
    "    total_dic_cnt = len(dic_data_df[dic_data_df['TAG']!=None])\n",
    "    print('Total words: {0:>5,}'.format(total_dic_cnt))\n",
    "    for label in label_lst:\n",
    "        try: \n",
    "            dic_lst = dic_data_df[dic_data_df['TAG']==label]['í‚¤ì›Œë“œ'].tolist()\n",
    "            dic_lst = list(set(dic_lst))\n",
    "            save_dic_df = pd.DataFrame(dic_lst, columns=[label_dict[label]])\n",
    "\n",
    "            # Create path if it does not exist\n",
    "            if not os.path.exists(tag_data_path):\n",
    "                os.makedirs(tag_data_path, exist_ok=True)\n",
    "                print(\"{} -- Folder create complete \\n\".format(tag_data_path))\n",
    "\n",
    "            save_dic_df.to_excel(tag_data_path+label_dict[label]+\".xlsx\")\n",
    "            print('tag : {} | cnt : {}'.format(label_dict[label], len(dic_lst)))\n",
    "        except:\n",
    "            print(\"error occurred while processing label \" + label_dict[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 117,692\n",
      "./tag_data/ -- Folder create complete \n",
      "\n",
      "tag : ORG-C | cnt : 2248\n",
      "tag : TE-C | cnt : 1016\n",
      "tag : MAT | cnt : 951\n",
      "tag : COM-C | cnt : 2785\n",
      "tag : EVT-C | cnt : 3448\n",
      "tag : ISS | cnt : 444\n",
      "tag : POL-C | cnt : 608\n",
      "tag : PRD | cnt : 845\n"
     ]
    }
   ],
   "source": [
    "make_ref_dic(label_lst, tag_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Merge DX tag -> Chem tag  \n",
    "\\- BM, LOC tagë“¤ì€ íŒŒì¼ íƒìƒ‰ì°½ì—ì„œ manually ë³µë¶™í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CO (dx) + COM-C (chem) -> COM\n",
    "com = set(pd.read_excel(tag_data_path+'COM-C.xlsx')['COM-C'].tolist())\n",
    "co = set(pd.read_excel(dx_path+'CO.xlsx')['CO'].tolist())\n",
    "com = co.union(com)\n",
    "pd.DataFrame(list(com), columns=['COM']).to_excel(tag_data_path+'COM.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVT (dx) + EVT-C (chem) -> EVT\n",
    "evt = set(pd.read_excel(tag_data_path+'EVT-C.xlsx')['EVT-C'].tolist())\n",
    "evt_dx = set(pd.read_excel(dx_path+'EVT.xlsx')['EVT'].tolist())\n",
    "evt = evt.union(evt_dx)\n",
    "pd.DataFrame(list(evt), columns=['EVT']).to_excel(tag_data_path+'EVT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORG (dx) + ORG-C (chem) -> ORG\n",
    "org = set(pd.read_excel(tag_data_path+'ORG-C.xlsx')['ORG-C'].tolist())\n",
    "org_dx = set(pd.read_excel(dx_path+'ORG.xlsx')['ORG'].tolist())\n",
    "org = org.union(org_dx)\n",
    "pd.DataFrame(list(org), columns=['ORG']).to_excel(tag_data_path+'ORG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PO (dx) + POL-C (chem) -> POL\n",
    "pol = set(pd.read_excel(tag_data_path+'POL-C.xlsx')['POL-C'].tolist())\n",
    "po_dx = set(pd.read_excel(dx_path+'PO.xlsx')['PO'].tolist())\n",
    "pol = pol.union(po_dx)\n",
    "pd.DataFrame(list(pol), columns=['POL']).to_excel(tag_data_path+'POL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TE (dx) + TE-C (chem) -> TE\n",
    "te = set(pd.read_excel(tag_data_path+'TE-C.xlsx')['TE-C'].tolist())\n",
    "te_dx = set(pd.read_excel(dx_path+'TE.xlsx')['TE'].tolist())\n",
    "te = te.union(te_dx)\n",
    "pd.DataFrame(list(te), columns=['TE']).to_excel(tag_data_path+'TE.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_label_lst = ['BM', 'COM', 'EVT', 'ISS', 'LOC', 'MAT', 'ORG', 'POL', 'PRD', 'TE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Generate train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load article from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>org</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20201231</td>\n",
       "      <td>ì„œìš¸ê²½ì œ</td>\n",
       "      <td>ê³¨ë“ íƒ€ì„ë„ ì§€ë‚˜ê³ ...ëª…ë¯¼í˜¸ ì„ ì› ì‹œì‹  ë°œê²¬</td>\n",
       "      <td>ì œì£¼ í•´ìƒì—ì„œ ì „ë³µëœ ì €ì¸ë§ ì–´ì„  â€˜32ëª…ë¯¼í˜¸â€™ì˜ ì„ ì› 1ëª…ì´ ì‚¬ë§í•œ ì±„ ë°œê²¬ëë‹¤....</td>\n",
       "      <td>viewer ì§€ë‚œ 30ì¼ ì œì£¼í•´ê²½ì´ ì œì£¼ í•´ìƒì—ì„œ ì „ë³µëœ ëª…ë¯¼í˜¸ì˜ ì„ ì›ìœ¼ë¡œ ì¶”ì •ë˜ëŠ”...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCQVY1P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20201231</td>\n",
       "      <td>ì„œìš¸ê²½ì œ</td>\n",
       "      <td>[ê¸°ì—…ê³µì‹œ 12ì›” 31ì¼]ABLë°”ì´ì˜¤, ë©´ì—­í•­ì•”ì œ ç¾ FDA 1ìƒ ê³„íš ì‹ ì²­ ë“±</td>\n",
       "      <td>&lt;ìœ ê°€ ì¦ê¶Œ&gt; \\n \\nâ–²ì•„ëª¨ë ˆG(002790)=ë°°ë™í˜„ ëŒ€í‘œì´ì‚¬ ì‚¬ì„ â–²ëŒ€ìš°ê±´ì„¤(0...</td>\n",
       "      <td>&lt; ì €ì‘ê¶Œì â“’ ì„œìš¸ê²½ì œ, ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€ &gt;\\n\\n=ë°°ë™í˜„ ëŒ€í‘œì´ì‚¬ ì‚¬...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCJX1OE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20201231</td>\n",
       "      <td>ë™ì•„ì¼ë³´</td>\n",
       "      <td>[ë‹¨ë…]ì€í‡´ ì•½ì† ì§€í‚¨ ì„œì •ì§„ â€œì›ê²©ì§„ë£Œ ìŠ¤íƒ€íŠ¸ì—… ë§¨ë•…ì„œ ì‹œì‘â€</td>\n",
       "      <td>ì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨ íšŒì¥(63)ì´ 31ì¼ íšŒì¥ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ë‹¤. â€˜ë‹¤ë¥¸ ì„ì›ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ...</td>\n",
       "      <td>ë™ì•„ì¼ë³´ DB.\\n\\nì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨ íšŒì¥(63)ì´ 31ì¼ íšŒì¥ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ë‹¤. â€˜...</td>\n",
       "      <td>https://www.donga.com/news/article/all/2020123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20201231</td>\n",
       "      <td>ì„œìš¸ê²½ì œ</td>\n",
       "      <td>ë§ˆì§€ë§‰ê¹Œì§€ ì½”ë¡œë‚˜ ì¹˜ë£Œì œ ê³µë“¤ì´ê³  ë– ë‚˜ëŠ” ì„œì •ì§„ íšŒì¥</td>\n",
       "      <td>ì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨(068270) ê·¸ë£¹ íšŒì¥ì´ 31ì¼ íšŒì¥ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ë‹¤. \\n \\nì§€...</td>\n",
       "      <td>viewer ì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨ê·¸ë£¹ íšŒì¥./ê¶Œìš±ê¸°ì\\n\\n&lt; ì €ì‘ê¶Œì â“’ ì„œìš¸ê²½ì œ, ë¬´...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCRA58P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20201231</td>\n",
       "      <td>ì„œìš¸ê²½ì œ</td>\n",
       "      <td>ì§€ìì²´ 'ì½”ë¡œë‚˜ ê·¹ë³µ ê²½ì œì‚´ë¦¬ê¸°' ì„±ê³µí”Œëœ ì§ ë‹¤</td>\n",
       "      <td>ì „ëŒ€ë¯¸ë¬¸ì˜ ì‹ ì¢… ì½”ë¡œë‚˜ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì¦(ì½”ë¡œë‚˜19) ëŒ€ìœ í–‰ì´ ì „êµ­ì„ ê°•íƒ€í•˜ë©´ì„œ 202...</td>\n",
       "      <td>viewer ì‹ ì¶•ë…„(è¾›ä¸‘å¹´)ì˜ íƒœì–‘ì´ ê°•ì›ë„ ê°•ë¦‰ì‹œ ì‚¬ì²œì§„í•´ë³€ì˜ ìˆ˜í‰ì„  ë„ˆë¨¸ë¡œ í˜ì°¨...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCBVQBL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date source                                         title  \\\n",
       "0  20201231   ì„œìš¸ê²½ì œ                      ê³¨ë“ íƒ€ì„ë„ ì§€ë‚˜ê³ ...ëª…ë¯¼í˜¸ ì„ ì› ì‹œì‹  ë°œê²¬   \n",
       "1  20201231   ì„œìš¸ê²½ì œ  [ê¸°ì—…ê³µì‹œ 12ì›” 31ì¼]ABLë°”ì´ì˜¤, ë©´ì—­í•­ì•”ì œ ç¾ FDA 1ìƒ ê³„íš ì‹ ì²­ ë“±   \n",
       "2  20201231   ë™ì•„ì¼ë³´           [ë‹¨ë…]ì€í‡´ ì•½ì† ì§€í‚¨ ì„œì •ì§„ â€œì›ê²©ì§„ë£Œ ìŠ¤íƒ€íŠ¸ì—… ë§¨ë•…ì„œ ì‹œì‘â€   \n",
       "3  20201231   ì„œìš¸ê²½ì œ                 ë§ˆì§€ë§‰ê¹Œì§€ ì½”ë¡œë‚˜ ì¹˜ë£Œì œ ê³µë“¤ì´ê³  ë– ë‚˜ëŠ” ì„œì •ì§„ íšŒì¥   \n",
       "4  20201231   ì„œìš¸ê²½ì œ                    ì§€ìì²´ 'ì½”ë¡œë‚˜ ê·¹ë³µ ê²½ì œì‚´ë¦¬ê¸°' ì„±ê³µí”Œëœ ì§ ë‹¤   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ì œì£¼ í•´ìƒì—ì„œ ì „ë³µëœ ì €ì¸ë§ ì–´ì„  â€˜32ëª…ë¯¼í˜¸â€™ì˜ ì„ ì› 1ëª…ì´ ì‚¬ë§í•œ ì±„ ë°œê²¬ëë‹¤....   \n",
       "1  <ìœ ê°€ ì¦ê¶Œ> \\n \\nâ–²ì•„ëª¨ë ˆG(002790)=ë°°ë™í˜„ ëŒ€í‘œì´ì‚¬ ì‚¬ì„ â–²ëŒ€ìš°ê±´ì„¤(0...   \n",
       "2  ì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨ íšŒì¥(63)ì´ 31ì¼ íšŒì¥ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ë‹¤. â€˜ë‹¤ë¥¸ ì„ì›ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ...   \n",
       "3  ì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨(068270) ê·¸ë£¹ íšŒì¥ì´ 31ì¼ íšŒì¥ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ë‹¤. \\n \\nì§€...   \n",
       "4  ì „ëŒ€ë¯¸ë¬¸ì˜ ì‹ ì¢… ì½”ë¡œë‚˜ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼ì¦(ì½”ë¡œë‚˜19) ëŒ€ìœ í–‰ì´ ì „êµ­ì„ ê°•íƒ€í•˜ë©´ì„œ 202...   \n",
       "\n",
       "                                                 org  \\\n",
       "0  viewer ì§€ë‚œ 30ì¼ ì œì£¼í•´ê²½ì´ ì œì£¼ í•´ìƒì—ì„œ ì „ë³µëœ ëª…ë¯¼í˜¸ì˜ ì„ ì›ìœ¼ë¡œ ì¶”ì •ë˜ëŠ”...   \n",
       "1  < ì €ì‘ê¶Œì â“’ ì„œìš¸ê²½ì œ, ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€ >\\n\\n=ë°°ë™í˜„ ëŒ€í‘œì´ì‚¬ ì‚¬...   \n",
       "2  ë™ì•„ì¼ë³´ DB.\\n\\nì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨ íšŒì¥(63)ì´ 31ì¼ íšŒì¥ì§ì—ì„œ ë¬¼ëŸ¬ë‚¬ë‹¤. â€˜...   \n",
       "3  viewer ì„œì •ì§„ ì…€íŠ¸ë¦¬ì˜¨ê·¸ë£¹ íšŒì¥./ê¶Œìš±ê¸°ì\\n\\n< ì €ì‘ê¶Œì â“’ ì„œìš¸ê²½ì œ, ë¬´...   \n",
       "4  viewer ì‹ ì¶•ë…„(è¾›ä¸‘å¹´)ì˜ íƒœì–‘ì´ ê°•ì›ë„ ê°•ë¦‰ì‹œ ì‚¬ì²œì§„í•´ë³€ì˜ ìˆ˜í‰ì„  ë„ˆë¨¸ë¡œ í˜ì°¨...   \n",
       "\n",
       "                                                link  \n",
       "0         http://www.sedaily.com/NewsView/1ZBWCQVY1P  \n",
       "1         http://www.sedaily.com/NewsView/1ZBWCJX1OE  \n",
       "2  https://www.donga.com/news/article/all/2020123...  \n",
       "3         http://www.sedaily.com/NewsView/1ZBWCRA58P  \n",
       "4         http://www.sedaily.com/NewsView/1ZBWCBVQBL  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(db_path + 'db.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''SElECT date, source, title, summary, org, link FROM article_kr_dx''')\n",
    "\n",
    "init_df = pd.DataFrame(c.fetchall(),\n",
    "                       columns=['date', 'source', 'title', 'summary', 'org', 'link'])\n",
    "filtered_df = init_df.drop_duplicates(['date', 'title'], keep='first')  \n",
    "sorted_df = filtered_df.sort_values(by='date', ascending=False)\n",
    "\n",
    "stop_word_df = pd.read_excel(stopwords_path+\"no_use_words_kr.xlsx\")\n",
    "stop_word_list = stop_word_df['word'].tolist()\n",
    "\n",
    "conn.close()\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       0\n",
       "source     0\n",
       "title      0\n",
       "summary    0\n",
       "org        0\n",
       "link       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = sorted_df.drop_duplicates(['org'], keep='first').reset_index(drop=True)\n",
    "article_df['org'] = article_df['org'].astype(str)\n",
    "article_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) trim too short/long articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17994\n",
      "77\n",
      "7012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10899"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by length -> ë¬¸ì¥ ê°œìˆ˜ 10000ê°œë¡œ ë§ì¶”ê¸°\n",
    "print(len(article_df))\n",
    "print(len(article_df[article_df['org'].map(len) < 200]))\n",
    "print(len(article_df[article_df['org'].map(len) > 1600]))\n",
    "# plt.hist(article_df['org'].map(len))\n",
    "\n",
    "fil_article_df = article_df[(article_df['org'].map(len) > 200) & (article_df['org'].map(len) < 1600)]\n",
    "fil_article_df = fil_article_df.drop_duplicates().reset_index(drop=True)\n",
    "len(fil_article_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10899"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_lst = list(set(fil_article_df['org'].tolist()))\n",
    "ar_lst = list(filter(lambda v: v, ar_lst)) # ê²°ì¸¡ì œê±°\n",
    "len(ar_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) split articles into sentences and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mailì œê±°\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URLì œê±°\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ã„±-ã…ã…-ã…£]+)'  # í•œê¸€ ììŒ, ëª¨ìŒ ì œê±°\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '\\n'         # \\n íƒœê·¸ ì œê±°\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[-=+,#/\\?:$@*\\\"â€»~&%ã†!ã€\\\\â€˜|\\(\\)\\[\\]\\<\\>`\\'â€¦ã€‹Â·â€œâ€â€™â€™â“’â”]'\n",
    "    # pattern = 'Â·â€œâ€˜â€™[-=+,#/\\\\?:$@*â€»~&%ã†!ã€\\â€˜'\"|\\(\\)\\[\\]\\<\\>`\\'â€¦ã€‹]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "\n",
    "    # text = text.replace(\"ë‹¤.\", \"ë‹¤.^\")\n",
    "    # text = text.replace(\"ë‹¤ .\", \"ë‹¤.^\")\n",
    "    # text = text.replace(\".\", \"_^\")\n",
    "    text = text.replace(\"â–²\",'')\n",
    "    text = text.replace(\"â–³\",'')\n",
    "    text = text.replace(\"â– \",'')\n",
    "    text = text.replace(\"â—†\",'')\n",
    "    text = text.replace(\"â–¶\",'')\n",
    "    text = text.replace(\"â—\",'')\n",
    "    text = text.replace(\"íšŒì¥\",'')\n",
    "    text = text.replace(\"íšŒì¥ë‹˜\",'')\n",
    "    text = text.replace(\"í•˜í˜„íšŒ\",'')\n",
    "    text = text.replace(\"êµ¬ê´‘ëª¨\",'')\n",
    "    text = text.replace(\"ê¶Œì˜ìˆ˜\",'')\n",
    "    \n",
    "    # pattern = '[^\\w\\s]'         # íŠ¹ìˆ˜ê¸°í˜¸ì œê±°\n",
    "    # text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = \"[A-Za-z0-9]\"     # ì˜ë¬¸/ìˆ«ìì œê±°\n",
    "    # pattern = \"[a-z]\"     # ì˜ë¬¸/ìˆ«ìì œê±°\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    \n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10899\n",
      "0.0005917549133300781 sec\n",
      "1000 / 10899\n",
      "0.8080470561981201 sec\n",
      "2000 / 10899\n",
      "1.6504991054534912 sec\n",
      "3000 / 10899\n",
      "2.4685349464416504 sec\n",
      "4000 / 10899\n",
      "3.2359139919281006 sec\n",
      "5000 / 10899\n",
      "4.048757076263428 sec\n",
      "6000 / 10899\n",
      "4.7641987800598145 sec\n",
      "7000 / 10899\n",
      "5.5324931144714355 sec\n",
      "8000 / 10899\n",
      "6.29544997215271 sec\n",
      "9000 / 10899\n",
      "7.044921875 sec\n",
      "10000 / 10899\n",
      "7.782399892807007 sec\n"
     ]
    }
   ],
   "source": [
    "sen_sum_lst=[]  ## sentence list\n",
    "ckpt1 = time()\n",
    "for i, ar in enumerate(ar_lst):  ## ê° ê¸°ì‚¬ì— ëŒ€í•´ì„œ\n",
    "    if i%1000==0:\n",
    "        print(i, \"/\", len(ar_lst))\n",
    "        print(time()-ckpt1, \"sec\")    \n",
    "    split_ar = sent_tokenize(ar)  ## ë¬¸ì¥ë‹¨ìœ„ë¡œ split\n",
    "    for temp_ar in split_ar:  ## ê° ë¬¸ì¥ë³„ë¡œ\n",
    "        temp_ar = clean_str(temp_ar)\n",
    "        temp_ar = temp_ar.strip()  ## ì•ë’¤ whitespace ì œê±°\n",
    "        sen_sum_lst.append(temp_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104925"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_sum_df = pd.DataFrame(sen_sum_lst, columns=['sentens'])\n",
    "len(sen_sum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100622\n",
      "94294\n"
     ]
    }
   ],
   "source": [
    "fin_sen_sum_df=sen_sum_df[sen_sum_df['sentens'].map(len) > 15]\n",
    "fin_sen_sum_df=fin_sen_sum_df[fin_sen_sum_df['sentens'].map(len) < 800]\n",
    "print(len(fin_sen_sum_df))\n",
    "fin_sen_sum_df=fin_sen_sum_df.drop_duplicates(['sentens'], keep='first')\n",
    "print(len(fin_sen_sum_df))\n",
    "# fin_sen_sum_lst=fin_sen_sum_df.sentens.tolist()\n",
    "# fin_sen_sum_df.to_excel(tag_data_path+'fin_sen_tst.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) tag and generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM cnt : 637\n",
      "value not str type : 6528870487 from COM\n",
      "COM cnt : 43284\n",
      "EVT cnt : 3568\n",
      "ISS cnt : 444\n",
      "LOC cnt : 450\n",
      "MAT cnt : 951\n",
      "ORG cnt : 2393\n",
      "POL cnt : 965\n",
      "PRD cnt : 845\n",
      "TE cnt : 1862\n"
     ]
    }
   ],
   "source": [
    "tag_word_sum_lst = []\n",
    "for label in tag_label_lst:\n",
    "    read_df = pd.read_excel(tag_data_path+label+'.xlsx')[label]\n",
    "    read_lst = read_df.tolist()\n",
    "    # str íƒ€ì…ì´ ì•„ë‹Œ ê°’ ì˜ˆì™¸ì²˜ë¦¬\n",
    "    for i in read_lst:\n",
    "        if type(i) is not str:\n",
    "            print('value not str type : ' + str(i) + ' from ' + label)\n",
    "            read_lst.remove(i)\n",
    "    tag_word_sum_lst.append(read_lst)\n",
    "    print(label+' cnt : '+str(len(read_lst)))\n",
    "    twitter.add_dictionary(read_lst, 'Noun')\n",
    "\n",
    "# íƒìƒ‰ íš¨ìœ¨ì„±ì„ ìœ„í•´ set structureë¡œ ë³€í™˜\n",
    "BM_lst= set(pd.read_excel(tag_data_path+'BM.xlsx')['BM'].tolist())\n",
    "COM_lst= set(pd.read_excel(tag_data_path+'COM.xlsx')['COM'].tolist())\n",
    "ISS_lst= set(pd.read_excel(tag_data_path+'ISS.xlsx')['ISS'].tolist())\n",
    "MAT_lst= set(pd.read_excel(tag_data_path+'MAT.xlsx')['MAT'].tolist())\n",
    "EVT_lst= set(pd.read_excel(tag_data_path+'EVT.xlsx')['EVT'].tolist())\n",
    "LOC_lst= set(pd.read_excel(tag_data_path+'LOC.xlsx')['LOC'].tolist())\n",
    "POL_lst= set(pd.read_excel(tag_data_path+'POL.xlsx')['POL'].tolist())\n",
    "PRD_lst= set(pd.read_excel(tag_data_path+'PRD.xlsx')['PRD'].tolist())\n",
    "ORG_lst= set(pd.read_excel(tag_data_path+'ORG.xlsx')['ORG'].tolist())\n",
    "TE_lst= set(pd.read_excel(tag_data_path+'TE.xlsx')['TE'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(word, pos, PRD_lst, MAT_lst, TE_lst, COM_lst, ISS_lst, EVT_lst, POL_lst, BM_lst, ORG_lst, LOC_lst):\n",
    "    if pos=='Noun' or pos == 'Number':\n",
    "        if word in PRD_lst:\n",
    "            return 'PRD-B'\n",
    "        elif word in MAT_lst:\n",
    "            return 'MAT-B'\n",
    "        elif word in TE_lst:\n",
    "            return 'TE-B'\n",
    "        elif word in COM_lst:\n",
    "            return 'COM-B'\n",
    "        elif word in ISS_lst:\n",
    "            return 'ISS-B'\n",
    "        elif word in EVT_lst:\n",
    "            return 'EVT-B'\n",
    "        elif word in POL_lst:\n",
    "            return 'POL-B'\n",
    "        elif word in BM_lst:\n",
    "            return 'BM-B'\n",
    "        elif word in ORG_lst:\n",
    "            return 'ORG-B'\n",
    "        elif word in LOC_lst:\n",
    "            return 'LOC-B'\n",
    "        elif pos == 'Number':\n",
    "            return 'NUM-B'\n",
    "#         elif word == \"_\":\n",
    "#             return \"_\"\n",
    "        else:\n",
    "            return 'O'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89728\n"
     ]
    }
   ],
   "source": [
    "input_sen=fin_sen_sum_df['sentens'][fin_sen_sum_df['sentens'].apply(lambda x: len(x)) <= 256].tolist() # BERT ê³„ì‚°ì†ë„ ê³ ë ¤\n",
    "print(len(input_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 89728\n",
      "0.0011289119720458984 sec\n",
      "10000 / 89728\n",
      "77.66743397712708 sec\n",
      "20000 / 89728\n",
      "151.76407599449158 sec\n",
      "30000 / 89728\n",
      "227.3166539669037 sec\n",
      "40000 / 89728\n",
      "308.53064489364624 sec\n",
      "50000 / 89728\n",
      "381.66545605659485 sec\n",
      "60000 / 89728\n",
      "487.33572793006897 sec\n",
      "70000 / 89728\n",
      "584.0805869102478 sec\n",
      "80000 / 89728\n",
      "662.1295757293701 sec\n"
     ]
    }
   ],
   "source": [
    "tagging_result_df=pd.DataFrame()\n",
    "tagging_result_df=pd.DataFrame(input_sen, columns=['sentens'])\n",
    "tagging_label_lst=[]\n",
    "tagging_sen_lst=[]\n",
    "\n",
    "ckpt2 = time()\n",
    "for i in range(len(input_sen)): \n",
    "    if i%10000==0:\n",
    "        print(i, \"/\", len(input_sen))\n",
    "        print(time()-ckpt2, \"sec\")\n",
    "    temp_df=pd.DataFrame(twitter.pos(input_sen[i]), columns=['word','pos'])\n",
    "    temp_df[\"tag\"] = temp_df.apply(lambda x : tag(x[\"word\"], x[\"pos\"], PRD_lst, MAT_lst, TE_lst, COM_lst, ISS_lst, EVT_lst, POL_lst, BM_lst, ORG_lst, LOC_lst) , axis = 1 )\n",
    "    # temp_df.reset_index(drop=True, inplace=True)\n",
    "    # print(temp_df)\n",
    "    temp_label_lst=temp_df['tag'].tolist()\n",
    "    temp_label_lst=' '.join(temp_label_lst)\n",
    "    tagging_label_lst.append(temp_label_lst)\n",
    "    \n",
    "    temp_word_lst=temp_df['word'].tolist()\n",
    "    temp_word_lst=' '.join(temp_word_lst)\n",
    "    tagging_sen_lst.append(temp_word_lst)\n",
    "    \n",
    "tagging_result_df['label']=pd.DataFrame(tagging_label_lst)\n",
    "tagging_result_df['sentens']=pd.DataFrame(tagging_sen_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë°ì´í„° ê°œìˆ˜ 89728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>í•˜ ì§€ë§Œ í–‰ì •ëª…ë ¹ ì„œëª… ìœ¼ë¡œ ë¯¸êµ­ ì—ì„œ ìƒì‚° ë˜ëŠ” ë°±ì‹  ì´ í•´ì™¸ ì— ìˆ˜ì¶œ ë˜ëŠ” ì‹œê°„...</td>\n",
       "      <td>O O EVT-B O O LOC-B O O O COM-B O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>í•œêµ­ ë„ ìƒí™© ì— ë§ëŠ” ì‹¤ë¦¬ì½˜ë°¸ë¦¬ ì¡°ì„± ì— ë‚˜ì„°ë‹¤ .</td>\n",
       "      <td>O O O O O COM-B COM-B O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ì§€ì—­ ì˜ ëŒ€í‘œ ê±°ì  ì‚°ì—… ë‹¨ì§€ ì™€ ì¸ê·¼ ì˜ ì—¬ëŸ¬ ì‚°ë‹¨ ì„ ë¬¶ì–´ ì‚°ì—… í˜ì‹  ê³µê°„ ìœ¼ë¡œ...</td>\n",
       "      <td>O O O O O O O O O O O O O O COM-B O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ğŸ“ ê´€ë ¨ í†µê³„ ìë£Œ ë‹¤ìš´ë¡œë“œ ê³µëª¨ ì£¼ í‰ê·  ì£¼ê°€ ìƒìŠ¹ë¥  ê³µëª¨ ê°€ ê²© ì´ í¬ë§ ê°€ê²© ...</td>\n",
       "      <td>O O O O O O O O O EVT-B O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ê° ë¶€ì²˜ ëŠ” ë‚´ë…„ ì˜ˆì‚° ìš”êµ¬ ì˜ ê°€ì´ë“œë¼ì¸ ìœ¼ë¡œ ì´ë¥¼ í™œìš© í•˜ê²Œ ëœë‹¤ .</td>\n",
       "      <td>O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentens  \\\n",
       "0  í•˜ ì§€ë§Œ í–‰ì •ëª…ë ¹ ì„œëª… ìœ¼ë¡œ ë¯¸êµ­ ì—ì„œ ìƒì‚° ë˜ëŠ” ë°±ì‹  ì´ í•´ì™¸ ì— ìˆ˜ì¶œ ë˜ëŠ” ì‹œê°„...   \n",
       "1                      í•œêµ­ ë„ ìƒí™© ì— ë§ëŠ” ì‹¤ë¦¬ì½˜ë°¸ë¦¬ ì¡°ì„± ì— ë‚˜ì„°ë‹¤ .   \n",
       "2  ì§€ì—­ ì˜ ëŒ€í‘œ ê±°ì  ì‚°ì—… ë‹¨ì§€ ì™€ ì¸ê·¼ ì˜ ì—¬ëŸ¬ ì‚°ë‹¨ ì„ ë¬¶ì–´ ì‚°ì—… í˜ì‹  ê³µê°„ ìœ¼ë¡œ...   \n",
       "3  ğŸ“ ê´€ë ¨ í†µê³„ ìë£Œ ë‹¤ìš´ë¡œë“œ ê³µëª¨ ì£¼ í‰ê·  ì£¼ê°€ ìƒìŠ¹ë¥  ê³µëª¨ ê°€ ê²© ì´ í¬ë§ ê°€ê²© ...   \n",
       "4           ê° ë¶€ì²˜ ëŠ” ë‚´ë…„ ì˜ˆì‚° ìš”êµ¬ ì˜ ê°€ì´ë“œë¼ì¸ ìœ¼ë¡œ ì´ë¥¼ í™œìš© í•˜ê²Œ ëœë‹¤ .   \n",
       "\n",
       "                                               label  \n",
       "0  O O EVT-B O O LOC-B O O O COM-B O O O O O O O ...  \n",
       "1                        O O O O O COM-B COM-B O O O  \n",
       "2  O O O O O O O O O O O O O O COM-B O O O O O O ...  \n",
       "3  O O O O O O O O O EVT-B O O O O O O O O O O O ...  \n",
       "4                        O O O O O O O O O O O O O O  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_result_df=tagging_result_df.sample(frac=1).reset_index(drop=True)\n",
    "print('ì „ì²´ ë°ì´í„° ê°œìˆ˜',len(shuffled_result_df))\n",
    "shuffled_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chem_data/ -- Folder create complete \n",
      "\n",
      "train_set ê°œìˆ˜ 71782\n",
      "test_set ê°œìˆ˜ 17946\n"
     ]
    }
   ],
   "source": [
    "# Create path if it does not exist\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(data_path))\n",
    "\n",
    "train_set = shuffled_result_df.sample(frac=0.8, random_state=2021)\n",
    "print('train_set ê°œìˆ˜', len(train_set))\n",
    "train_set.to_excel(data_path+'train_chem.xlsx')\n",
    "\n",
    "test_set = shuffled_result_df.drop(train_set.index)\n",
    "print('test_set ê°œìˆ˜', len(test_set))\n",
    "test_set.to_excel(data_path+'test_chem.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

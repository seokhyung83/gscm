{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LG화학 학습데이터 생성  \n",
    "lg_chem 폴더 내에 db.sqlite, 불용어 파일, 태그 파일(tag.xlsx로 파일명 변환), dx_tag 폴더를 넣은 상태에서 돌리면 결과 재현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinho.lee\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "# from pororo import Pororo\n",
    "# prr = Pororo(task=\"tokenization\", lang=\"ko\", model=\"mecab_ko\")\n",
    "# prr = Pororo(task=\"tokenization\", lang=\"ko\", model=\"sent_ko\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 태그 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path        = './'\n",
    "dx_path        = './dx_tag/'\n",
    "data_path      = \"./chem_data/\"\n",
    "tag_data_path  = \"./tag_data/\"\n",
    "stopwords_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data_df = pd.read_excel('./tag.xlsx')\n",
    "tag_data_df = tag_data_df.drop_duplicates(['키워드'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lst = tag_data_df['TAG'][-tag_data_df['TAG'].isna()].unique().tolist()\n",
    "# ['기관', '기술', '소재', '업체', '이벤트', '이슈', '정책', '제품']\n",
    "\n",
    "global label_dict # 함수 내에서 사용. LG화학의 경우 태그가 한글로 되어있어서 변경하고자 함수를 수정하였습니다.\n",
    "# 'LABEL-C'은 화학 고유의 태그이고, DX tag와 merge된 '-C'가 빠진 파일명을 가질 예정입니다.\n",
    "label_dict = {'기관' : 'ORG-C', '기술' : 'TE-C', '소재' : 'MAT', '업체' : 'COM-C', '이벤트' : 'EVT-C', '이슈' : 'ISS', '정책' : 'POL-C', '제품' : 'PRD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ref_dic(label_lst, dic_data_df):\n",
    "    total_dic_cnt = len(dic_data_df[dic_data_df['TAG']!=None])\n",
    "    print('Total words: {0:>5,}'.format(total_dic_cnt))\n",
    "    for label in label_lst:\n",
    "        try: \n",
    "            dic_lst = dic_data_df[dic_data_df['TAG']==label]['키워드'].tolist()\n",
    "            dic_lst = list(set(dic_lst))\n",
    "            save_dic_df = pd.DataFrame(dic_lst, columns=[label_dict[label]])\n",
    "\n",
    "            # Create path if it does not exist\n",
    "            if not os.path.exists(tag_data_path):\n",
    "                os.makedirs(tag_data_path, exist_ok=True)\n",
    "                print(\"{} -- Folder create complete \\n\".format(tag_data_path))\n",
    "\n",
    "            save_dic_df.to_excel(tag_data_path+label_dict[label]+\".xlsx\")\n",
    "            print('tag : {} | cnt : {}'.format(label_dict[label], len(dic_lst)))\n",
    "        except:\n",
    "            print(\"error occurred while processing label \" + label_dict[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 117,692\n",
      "./tag_data/ -- Folder create complete \n",
      "\n",
      "tag : ORG-C | cnt : 2248\n",
      "tag : TE-C | cnt : 1016\n",
      "tag : MAT | cnt : 951\n",
      "tag : COM-C | cnt : 2785\n",
      "tag : EVT-C | cnt : 3448\n",
      "tag : ISS | cnt : 444\n",
      "tag : POL-C | cnt : 608\n",
      "tag : PRD | cnt : 845\n"
     ]
    }
   ],
   "source": [
    "make_ref_dic(label_lst, tag_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Merge DX tag -> Chem tag  \n",
    "\\- BM, LOC tag들은 파일 탐색창에서 manually 복붙하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CO (dx) + COM-C (chem) -> COM\n",
    "com = set(pd.read_excel(tag_data_path+'COM-C.xlsx')['COM-C'].tolist())\n",
    "co = set(pd.read_excel(dx_path+'CO.xlsx')['CO'].tolist())\n",
    "com = co.union(com)\n",
    "pd.DataFrame(list(com), columns=['COM']).to_excel(tag_data_path+'COM.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVT (dx) + EVT-C (chem) -> EVT\n",
    "evt = set(pd.read_excel(tag_data_path+'EVT-C.xlsx')['EVT-C'].tolist())\n",
    "evt_dx = set(pd.read_excel(dx_path+'EVT.xlsx')['EVT'].tolist())\n",
    "evt = evt.union(evt_dx)\n",
    "pd.DataFrame(list(evt), columns=['EVT']).to_excel(tag_data_path+'EVT.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORG (dx) + ORG-C (chem) -> ORG\n",
    "org = set(pd.read_excel(tag_data_path+'ORG-C.xlsx')['ORG-C'].tolist())\n",
    "org_dx = set(pd.read_excel(dx_path+'ORG.xlsx')['ORG'].tolist())\n",
    "org = org.union(org_dx)\n",
    "pd.DataFrame(list(org), columns=['ORG']).to_excel(tag_data_path+'ORG.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PO (dx) + POL-C (chem) -> POL\n",
    "pol = set(pd.read_excel(tag_data_path+'POL-C.xlsx')['POL-C'].tolist())\n",
    "po_dx = set(pd.read_excel(dx_path+'PO.xlsx')['PO'].tolist())\n",
    "pol = pol.union(po_dx)\n",
    "pd.DataFrame(list(pol), columns=['POL']).to_excel(tag_data_path+'POL.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TE (dx) + TE-C (chem) -> TE\n",
    "te = set(pd.read_excel(tag_data_path+'TE-C.xlsx')['TE-C'].tolist())\n",
    "te_dx = set(pd.read_excel(dx_path+'TE.xlsx')['TE'].tolist())\n",
    "te = te.union(te_dx)\n",
    "pd.DataFrame(list(te), columns=['TE']).to_excel(tag_data_path+'TE.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_label_lst = ['BM', 'COM', 'EVT', 'ISS', 'LOC', 'MAT', 'ORG', 'POL', 'PRD', 'TE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Generate train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load article from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>org</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20201231</td>\n",
       "      <td>서울경제</td>\n",
       "      <td>골든타임도 지나고...명민호 선원 시신 발견</td>\n",
       "      <td>제주 해상에서 전복된 저인망 어선 ‘32명민호’의 선원 1명이 사망한 채 발견됐다....</td>\n",
       "      <td>viewer 지난 30일 제주해경이 제주 해상에서 전복된 명민호의 선원으로 추정되는...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCQVY1P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20201231</td>\n",
       "      <td>서울경제</td>\n",
       "      <td>[기업공시 12월 31일]ABL바이오, 면역항암제 美 FDA 1상 계획 신청 등</td>\n",
       "      <td>&lt;유가 증권&gt; \\n \\n▲아모레G(002790)=배동현 대표이사 사임 ▲대우건설(0...</td>\n",
       "      <td>&lt; 저작권자 ⓒ 서울경제, 무단 전재 및 재배포 금지 &gt;\\n\\n=배동현 대표이사 사...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCJX1OE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20201231</td>\n",
       "      <td>동아일보</td>\n",
       "      <td>[단독]은퇴 약속 지킨 서정진 “원격진료 스타트업 맨땅서 시작”</td>\n",
       "      <td>서정진 셀트리온 회장(63)이 31일 회장직에서 물러났다. ‘다른 임원과 마찬가지로...</td>\n",
       "      <td>동아일보 DB.\\n\\n서정진 셀트리온 회장(63)이 31일 회장직에서 물러났다. ‘...</td>\n",
       "      <td>https://www.donga.com/news/article/all/2020123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20201231</td>\n",
       "      <td>서울경제</td>\n",
       "      <td>마지막까지 코로나 치료제 공들이고 떠나는 서정진 회장</td>\n",
       "      <td>서정진 셀트리온(068270) 그룹 회장이 31일 회장직에서 물러났다. \\n \\n지...</td>\n",
       "      <td>viewer 서정진 셀트리온그룹 회장./권욱기자\\n\\n&lt; 저작권자 ⓒ 서울경제, 무...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCRA58P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20201231</td>\n",
       "      <td>서울경제</td>\n",
       "      <td>지자체 '코로나 극복 경제살리기' 성공플랜 짠다</td>\n",
       "      <td>전대미문의 신종 코로나바이러스 감염증(코로나19) 대유행이 전국을 강타하면서 202...</td>\n",
       "      <td>viewer 신축년(辛丑年)의 태양이 강원도 강릉시 사천진해변의 수평선 너머로 힘차...</td>\n",
       "      <td>http://www.sedaily.com/NewsView/1ZBWCBVQBL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date source                                         title  \\\n",
       "0  20201231   서울경제                      골든타임도 지나고...명민호 선원 시신 발견   \n",
       "1  20201231   서울경제  [기업공시 12월 31일]ABL바이오, 면역항암제 美 FDA 1상 계획 신청 등   \n",
       "2  20201231   동아일보           [단독]은퇴 약속 지킨 서정진 “원격진료 스타트업 맨땅서 시작”   \n",
       "3  20201231   서울경제                 마지막까지 코로나 치료제 공들이고 떠나는 서정진 회장   \n",
       "4  20201231   서울경제                    지자체 '코로나 극복 경제살리기' 성공플랜 짠다   \n",
       "\n",
       "                                             summary  \\\n",
       "0  제주 해상에서 전복된 저인망 어선 ‘32명민호’의 선원 1명이 사망한 채 발견됐다....   \n",
       "1  <유가 증권> \\n \\n▲아모레G(002790)=배동현 대표이사 사임 ▲대우건설(0...   \n",
       "2  서정진 셀트리온 회장(63)이 31일 회장직에서 물러났다. ‘다른 임원과 마찬가지로...   \n",
       "3  서정진 셀트리온(068270) 그룹 회장이 31일 회장직에서 물러났다. \\n \\n지...   \n",
       "4  전대미문의 신종 코로나바이러스 감염증(코로나19) 대유행이 전국을 강타하면서 202...   \n",
       "\n",
       "                                                 org  \\\n",
       "0  viewer 지난 30일 제주해경이 제주 해상에서 전복된 명민호의 선원으로 추정되는...   \n",
       "1  < 저작권자 ⓒ 서울경제, 무단 전재 및 재배포 금지 >\\n\\n=배동현 대표이사 사...   \n",
       "2  동아일보 DB.\\n\\n서정진 셀트리온 회장(63)이 31일 회장직에서 물러났다. ‘...   \n",
       "3  viewer 서정진 셀트리온그룹 회장./권욱기자\\n\\n< 저작권자 ⓒ 서울경제, 무...   \n",
       "4  viewer 신축년(辛丑年)의 태양이 강원도 강릉시 사천진해변의 수평선 너머로 힘차...   \n",
       "\n",
       "                                                link  \n",
       "0         http://www.sedaily.com/NewsView/1ZBWCQVY1P  \n",
       "1         http://www.sedaily.com/NewsView/1ZBWCJX1OE  \n",
       "2  https://www.donga.com/news/article/all/2020123...  \n",
       "3         http://www.sedaily.com/NewsView/1ZBWCRA58P  \n",
       "4         http://www.sedaily.com/NewsView/1ZBWCBVQBL  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(db_path + 'db.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''SElECT date, source, title, summary, org, link FROM article_kr_dx''')\n",
    "\n",
    "init_df = pd.DataFrame(c.fetchall(),\n",
    "                       columns=['date', 'source', 'title', 'summary', 'org', 'link'])\n",
    "filtered_df = init_df.drop_duplicates(['date', 'title'], keep='first')  \n",
    "sorted_df = filtered_df.sort_values(by='date', ascending=False)\n",
    "\n",
    "stop_word_df = pd.read_excel(stopwords_path+\"no_use_words_kr.xlsx\")\n",
    "stop_word_list = stop_word_df['word'].tolist()\n",
    "\n",
    "conn.close()\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       0\n",
       "source     0\n",
       "title      0\n",
       "summary    0\n",
       "org        0\n",
       "link       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df = sorted_df.drop_duplicates(['org'], keep='first').reset_index(drop=True)\n",
    "article_df['org'] = article_df['org'].astype(str)\n",
    "article_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) trim too short/long articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17994\n",
      "77\n",
      "7012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10899"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by length -> 문장 개수 10000개로 맞추기\n",
    "print(len(article_df))\n",
    "print(len(article_df[article_df['org'].map(len) < 200]))\n",
    "print(len(article_df[article_df['org'].map(len) > 1600]))\n",
    "# plt.hist(article_df['org'].map(len))\n",
    "\n",
    "fil_article_df = article_df[(article_df['org'].map(len) > 200) & (article_df['org'].map(len) < 1600)]\n",
    "fil_article_df = fil_article_df.drop_duplicates().reset_index(drop=True)\n",
    "len(fil_article_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10899"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_lst = list(set(fil_article_df['org'].tolist()))\n",
    "ar_lst = list(filter(lambda v: v, ar_lst)) # 결측제거\n",
    "len(ar_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) split articles into sentences and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'  # 한글 자음, 모음 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '<[^>]*>'         # HTML 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '\\n'         # \\n 태그 제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '[-=+,#/\\?:$@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》·“”’’ⓒ━]'\n",
    "    # pattern = '·“‘’[-=+,#/\\\\?:$@*※~&%ㆍ!』\\‘'\"|\\(\\)\\[\\]\\<\\>`\\'…》]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "\n",
    "    # text = text.replace(\"다.\", \"다.^\")\n",
    "    # text = text.replace(\"다 .\", \"다.^\")\n",
    "    # text = text.replace(\".\", \"_^\")\n",
    "    text = text.replace(\"▲\",'')\n",
    "    text = text.replace(\"△\",'')\n",
    "    text = text.replace(\"■\",'')\n",
    "    text = text.replace(\"◆\",'')\n",
    "    text = text.replace(\"▶\",'')\n",
    "    text = text.replace(\"●\",'')\n",
    "    text = text.replace(\"회장\",'')\n",
    "    text = text.replace(\"회장님\",'')\n",
    "    text = text.replace(\"하현회\",'')\n",
    "    text = text.replace(\"구광모\",'')\n",
    "    text = text.replace(\"권영수\",'')\n",
    "    \n",
    "    # pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    # text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = \"[A-Za-z0-9]\"     # 영문/숫자제거\n",
    "    # pattern = \"[a-z]\"     # 영문/숫자제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    \n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10899\n",
      "0.0005917549133300781 sec\n",
      "1000 / 10899\n",
      "0.8080470561981201 sec\n",
      "2000 / 10899\n",
      "1.6504991054534912 sec\n",
      "3000 / 10899\n",
      "2.4685349464416504 sec\n",
      "4000 / 10899\n",
      "3.2359139919281006 sec\n",
      "5000 / 10899\n",
      "4.048757076263428 sec\n",
      "6000 / 10899\n",
      "4.7641987800598145 sec\n",
      "7000 / 10899\n",
      "5.5324931144714355 sec\n",
      "8000 / 10899\n",
      "6.29544997215271 sec\n",
      "9000 / 10899\n",
      "7.044921875 sec\n",
      "10000 / 10899\n",
      "7.782399892807007 sec\n"
     ]
    }
   ],
   "source": [
    "sen_sum_lst=[]  ## sentence list\n",
    "ckpt1 = time()\n",
    "for i, ar in enumerate(ar_lst):  ## 각 기사에 대해서\n",
    "    if i%1000==0:\n",
    "        print(i, \"/\", len(ar_lst))\n",
    "        print(time()-ckpt1, \"sec\")    \n",
    "    split_ar = sent_tokenize(ar)  ## 문장단위로 split\n",
    "    for temp_ar in split_ar:  ## 각 문장별로\n",
    "        temp_ar = clean_str(temp_ar)\n",
    "        temp_ar = temp_ar.strip()  ## 앞뒤 whitespace 제거\n",
    "        sen_sum_lst.append(temp_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104925"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_sum_df = pd.DataFrame(sen_sum_lst, columns=['sentens'])\n",
    "len(sen_sum_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100622\n",
      "94294\n"
     ]
    }
   ],
   "source": [
    "fin_sen_sum_df=sen_sum_df[sen_sum_df['sentens'].map(len) > 15]\n",
    "fin_sen_sum_df=fin_sen_sum_df[fin_sen_sum_df['sentens'].map(len) < 800]\n",
    "print(len(fin_sen_sum_df))\n",
    "fin_sen_sum_df=fin_sen_sum_df.drop_duplicates(['sentens'], keep='first')\n",
    "print(len(fin_sen_sum_df))\n",
    "# fin_sen_sum_lst=fin_sen_sum_df.sentens.tolist()\n",
    "# fin_sen_sum_df.to_excel(tag_data_path+'fin_sen_tst.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) tag and generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM cnt : 637\n",
      "value not str type : 6528870487 from COM\n",
      "COM cnt : 43284\n",
      "EVT cnt : 3568\n",
      "ISS cnt : 444\n",
      "LOC cnt : 450\n",
      "MAT cnt : 951\n",
      "ORG cnt : 2393\n",
      "POL cnt : 965\n",
      "PRD cnt : 845\n",
      "TE cnt : 1862\n"
     ]
    }
   ],
   "source": [
    "tag_word_sum_lst = []\n",
    "for label in tag_label_lst:\n",
    "    read_df = pd.read_excel(tag_data_path+label+'.xlsx')[label]\n",
    "    read_lst = read_df.tolist()\n",
    "    # str 타입이 아닌 값 예외처리\n",
    "    for i in read_lst:\n",
    "        if type(i) is not str:\n",
    "            print('value not str type : ' + str(i) + ' from ' + label)\n",
    "            read_lst.remove(i)\n",
    "    tag_word_sum_lst.append(read_lst)\n",
    "    print(label+' cnt : '+str(len(read_lst)))\n",
    "    twitter.add_dictionary(read_lst, 'Noun')\n",
    "\n",
    "# 탐색 효율성을 위해 set structure로 변환\n",
    "BM_lst= set(pd.read_excel(tag_data_path+'BM.xlsx')['BM'].tolist())\n",
    "COM_lst= set(pd.read_excel(tag_data_path+'COM.xlsx')['COM'].tolist())\n",
    "ISS_lst= set(pd.read_excel(tag_data_path+'ISS.xlsx')['ISS'].tolist())\n",
    "MAT_lst= set(pd.read_excel(tag_data_path+'MAT.xlsx')['MAT'].tolist())\n",
    "EVT_lst= set(pd.read_excel(tag_data_path+'EVT.xlsx')['EVT'].tolist())\n",
    "LOC_lst= set(pd.read_excel(tag_data_path+'LOC.xlsx')['LOC'].tolist())\n",
    "POL_lst= set(pd.read_excel(tag_data_path+'POL.xlsx')['POL'].tolist())\n",
    "PRD_lst= set(pd.read_excel(tag_data_path+'PRD.xlsx')['PRD'].tolist())\n",
    "ORG_lst= set(pd.read_excel(tag_data_path+'ORG.xlsx')['ORG'].tolist())\n",
    "TE_lst= set(pd.read_excel(tag_data_path+'TE.xlsx')['TE'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(word, pos, PRD_lst, MAT_lst, TE_lst, COM_lst, ISS_lst, EVT_lst, POL_lst, BM_lst, ORG_lst, LOC_lst):\n",
    "    if pos=='Noun' or pos == 'Number':\n",
    "        if word in PRD_lst:\n",
    "            return 'PRD-B'\n",
    "        elif word in MAT_lst:\n",
    "            return 'MAT-B'\n",
    "        elif word in TE_lst:\n",
    "            return 'TE-B'\n",
    "        elif word in COM_lst:\n",
    "            return 'COM-B'\n",
    "        elif word in ISS_lst:\n",
    "            return 'ISS-B'\n",
    "        elif word in EVT_lst:\n",
    "            return 'EVT-B'\n",
    "        elif word in POL_lst:\n",
    "            return 'POL-B'\n",
    "        elif word in BM_lst:\n",
    "            return 'BM-B'\n",
    "        elif word in ORG_lst:\n",
    "            return 'ORG-B'\n",
    "        elif word in LOC_lst:\n",
    "            return 'LOC-B'\n",
    "        elif pos == 'Number':\n",
    "            return 'NUM-B'\n",
    "#         elif word == \"_\":\n",
    "#             return \"_\"\n",
    "        else:\n",
    "            return 'O'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89728\n"
     ]
    }
   ],
   "source": [
    "input_sen=fin_sen_sum_df['sentens'][fin_sen_sum_df['sentens'].apply(lambda x: len(x)) <= 256].tolist() # BERT 계산속도 고려\n",
    "print(len(input_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 89728\n",
      "0.0011289119720458984 sec\n",
      "10000 / 89728\n",
      "77.66743397712708 sec\n",
      "20000 / 89728\n",
      "151.76407599449158 sec\n",
      "30000 / 89728\n",
      "227.3166539669037 sec\n",
      "40000 / 89728\n",
      "308.53064489364624 sec\n",
      "50000 / 89728\n",
      "381.66545605659485 sec\n",
      "60000 / 89728\n",
      "487.33572793006897 sec\n",
      "70000 / 89728\n",
      "584.0805869102478 sec\n",
      "80000 / 89728\n",
      "662.1295757293701 sec\n"
     ]
    }
   ],
   "source": [
    "tagging_result_df=pd.DataFrame()\n",
    "tagging_result_df=pd.DataFrame(input_sen, columns=['sentens'])\n",
    "tagging_label_lst=[]\n",
    "tagging_sen_lst=[]\n",
    "\n",
    "ckpt2 = time()\n",
    "for i in range(len(input_sen)): \n",
    "    if i%10000==0:\n",
    "        print(i, \"/\", len(input_sen))\n",
    "        print(time()-ckpt2, \"sec\")\n",
    "    temp_df=pd.DataFrame(twitter.pos(input_sen[i]), columns=['word','pos'])\n",
    "    temp_df[\"tag\"] = temp_df.apply(lambda x : tag(x[\"word\"], x[\"pos\"], PRD_lst, MAT_lst, TE_lst, COM_lst, ISS_lst, EVT_lst, POL_lst, BM_lst, ORG_lst, LOC_lst) , axis = 1 )\n",
    "    # temp_df.reset_index(drop=True, inplace=True)\n",
    "    # print(temp_df)\n",
    "    temp_label_lst=temp_df['tag'].tolist()\n",
    "    temp_label_lst=' '.join(temp_label_lst)\n",
    "    tagging_label_lst.append(temp_label_lst)\n",
    "    \n",
    "    temp_word_lst=temp_df['word'].tolist()\n",
    "    temp_word_lst=' '.join(temp_word_lst)\n",
    "    tagging_sen_lst.append(temp_word_lst)\n",
    "    \n",
    "tagging_result_df['label']=pd.DataFrame(tagging_label_lst)\n",
    "tagging_result_df['sentens']=pd.DataFrame(tagging_sen_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 개수 89728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>하 지만 행정명령 서명 으로 미국 에서 생산 되는 백신 이 해외 에 수출 되는 시간...</td>\n",
       "      <td>O O EVT-B O O LOC-B O O O COM-B O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>한국 도 상황 에 맞는 실리콘밸리 조성 에 나섰다 .</td>\n",
       "      <td>O O O O O COM-B COM-B O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>지역 의 대표 거점 산업 단지 와 인근 의 여러 산단 을 묶어 산업 혁신 공간 으로...</td>\n",
       "      <td>O O O O O O O O O O O O O O COM-B O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>📁 관련 통계 자료 다운로드 공모 주 평균 주가 상승률 공모 가 격 이 희망 가격 ...</td>\n",
       "      <td>O O O O O O O O O EVT-B O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>각 부처 는 내년 예산 요구 의 가이드라인 으로 이를 활용 하게 된다 .</td>\n",
       "      <td>O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentens  \\\n",
       "0  하 지만 행정명령 서명 으로 미국 에서 생산 되는 백신 이 해외 에 수출 되는 시간...   \n",
       "1                      한국 도 상황 에 맞는 실리콘밸리 조성 에 나섰다 .   \n",
       "2  지역 의 대표 거점 산업 단지 와 인근 의 여러 산단 을 묶어 산업 혁신 공간 으로...   \n",
       "3  📁 관련 통계 자료 다운로드 공모 주 평균 주가 상승률 공모 가 격 이 희망 가격 ...   \n",
       "4           각 부처 는 내년 예산 요구 의 가이드라인 으로 이를 활용 하게 된다 .   \n",
       "\n",
       "                                               label  \n",
       "0  O O EVT-B O O LOC-B O O O COM-B O O O O O O O ...  \n",
       "1                        O O O O O COM-B COM-B O O O  \n",
       "2  O O O O O O O O O O O O O O COM-B O O O O O O ...  \n",
       "3  O O O O O O O O O EVT-B O O O O O O O O O O O ...  \n",
       "4                        O O O O O O O O O O O O O O  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_result_df=tagging_result_df.sample(frac=1).reset_index(drop=True)\n",
    "print('전체 데이터 개수',len(shuffled_result_df))\n",
    "shuffled_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./chem_data/ -- Folder create complete \n",
      "\n",
      "train_set 개수 71782\n",
      "test_set 개수 17946\n"
     ]
    }
   ],
   "source": [
    "# Create path if it does not exist\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(data_path))\n",
    "\n",
    "train_set = shuffled_result_df.sample(frac=0.8, random_state=2021)\n",
    "print('train_set 개수', len(train_set))\n",
    "train_set.to_excel(data_path+'train_chem.xlsx')\n",
    "\n",
    "test_set = shuffled_result_df.drop(train_set.index)\n",
    "print('test_set 개수', len(test_set))\n",
    "test_set.to_excel(data_path+'test_chem.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_38",
   "language": "python",
   "name": "tf_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
